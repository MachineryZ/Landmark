{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FCN to detect landmark point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the data\n",
    "import networkx as nx\n",
    "from process_it import process_it\n",
    "import os\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import math        \n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import csv\n",
    "from random import randint, random, seed\n",
    "import time\n",
    "\n",
    "def state_to_numpy(state):\n",
    "    strlist = state.split()\n",
    "    val_list = [float(s) for s in strlist]\n",
    "    return np.array(val_list)\n",
    "\n",
    "def numpy_to_state(array):\n",
    "    state = \"\"\n",
    "    for i in range(len(array)):\n",
    "        state += str(array[i])+\" \"\n",
    "    return state\n",
    "\n",
    "def list_all_dir(data_dir):\n",
    "    task_dirs = os.listdir(data_dir)\n",
    "\n",
    "    list_dir = []\n",
    "    for task_dir in task_dirs:\n",
    "        env_dirs = os.listdir(data_dir+\"/\"+task_dir)\n",
    "        for env_dir in env_dirs:\n",
    "            list_dir.append(data_dir +\"/\"+ task_dir +\"/\"+ env_dir)\n",
    "    return list_dir  \n",
    "\n",
    "def detection_data(G, directory, grid, with_start_goal=False):\n",
    "    start = np.loadtxt(directory+\"/start_nodes.txt\")\n",
    "    goal = np.loadtxt(directory+\"/goal_nodes.txt\")\n",
    "    occ_grid = np.loadtxt(directory+\"/occ_grid.txt\")\n",
    "    path_nodes = []\n",
    "    occ = {} # dict\n",
    "    ini = {}\n",
    "    end = {}\n",
    "    i = 0\n",
    "    all_data = []\n",
    "    with open(directory + \"/path_nodes.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip('\\n')\n",
    "            \n",
    "            s = state_to_numpy(G.nodes[str(int(start[i]))]['state'])\n",
    "            g = state_to_numpy(G.nodes[str(int(goal[i]))]['state'])\n",
    "            og = occ_grid[i]\n",
    "            path_nodes = str(line).split(\",\")\n",
    "            # print(path_nodes)\n",
    "            for path_node in path_nodes:\n",
    "                if(path_node=='-1'):\n",
    "                    continue\n",
    "                \n",
    "                node_conf = state_to_numpy(G.nodes[path_node]['state'])\n",
    "                curr_node = np.array([])\n",
    "                \n",
    "                og = og.astype(np.int32)\n",
    "                str_og = numpy_to_state(og)\n",
    "                if str_og not in occ.keys():\n",
    "                    occ[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                if str_og not in ini.keys():\n",
    "                    ini[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                if str_og not in end.keys():\n",
    "                    end[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                occ[str_og][int(node_conf[0]*10*grid)][int(node_conf[1]*10*grid) + 1] = 1\n",
    "                ini[str_og][int(s[0]*10*grid)][int(s[1]*10*grid)] = 1\n",
    "                end[str_og][int(g[0]*10*grid)][int(g[1]*10*grid)] = 1\n",
    "            i += 1 \n",
    "    for element in occ:\n",
    "        tmp_occ = state_to_numpy(element)\n",
    "        tmp_occ = tmp_occ.astype(np.uint8)\n",
    "        tmp_occ = tmp_occ.reshape(10, 10)\n",
    "        tmp_occ = tmp_occ.repeat(grid, axis = 0)\n",
    "        tmp_occ = tmp_occ.repeat(grid, axis = 1)\n",
    "        \n",
    "        data = []\n",
    "        data.append(tmp_occ)\n",
    "        data.append(occ[element])\n",
    "        if with_start_goal == True:\n",
    "            data.append(ini[element])\n",
    "            data.append(end[element])\n",
    "        data = np.array(data)\n",
    "        all_data.append(data)\n",
    "    all_data = np.array(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.vgg import VGG\n",
    "\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCN16s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCNs(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n",
    "        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.9, b2=0.999, batch_size=16, eps=1e-08, lr=0.0001, max_epochs=1000, n_cpu=8)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--eps\", type=float, default=1e-8, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "#parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "#parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "#parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "#parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([863, 3, 160, 160])\n",
      "torch.Size([863, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DetectionSet(Dataset):\n",
    "    def __init__(self, data, with_start_goal=True, gaussian=False):\n",
    "        if with_start_goal == True:\n",
    "            \n",
    "            self.data = torch.FloatTensor(data[:,[0,2,3]])\n",
    "            self.label = torch.IntTensor(data[:,1])\n",
    "            print(self.data.shape)\n",
    "            print(self.label.shape)\n",
    "        else:\n",
    "            self.data = torch.FloatTensor(data[:,0])\n",
    "            self.label = torch.FloatTeensor(data[:, 1])\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "G = nx.read_graphml(\"graphs/dense_graph.graphml\")\n",
    "data_dir = \"dataset\"\n",
    "data = detection_data(G, data_dir, 16, with_start_goal = True)\n",
    "ratioTestTrain = 0.8\n",
    "numtotal = data.shape[0]\n",
    "train_data = data[0:int(numtotal*ratioTestTrain)]\n",
    "test_data = data[int(numtotal*ratioTestTrain):numtotal]\n",
    "train_set = DetectionSet(data = train_data, with_start_goal = True)\n",
    "train_loader = DataLoader(train_set, batch_size = opt.batch_size, shuffle = True)\n",
    "#test_set = DetectionSet(data = test_data, with_start_goal = True)\n",
    "#test_loader = DataLoader(test_set, batch_size = opt.batch_size, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG = VGGNet(pretrained = False)\n",
    "# FCN32s = FCN32s(pretrained_net = VGG, n_class = 2)\n",
    "# FCN16s = FCN16s(pretrained_net = VGG, n_class = 2)\n",
    "# FCN8s = FCN8s(pretrained_net = VGG, n_class = 2)\n",
    "# FCNs = FCNs(pretrained_net = VGG, n_class = 2)\n",
    "\n",
    "\n",
    "VGG = VGGNet(pretrained = False)\n",
    "net = FCN32s(pretrained_net = VGG, n_class = 1)\n",
    "\n",
    "resume = None\n",
    "# resume = \"\"\n",
    "if resume == None:\n",
    "    i = 9\n",
    "else:\n",
    "    net.load_state_dict(torch.load(resume))\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "net.train()\n",
    "optimizer = optim.Adam(net.parameters(), lr=opt.lr, \n",
    "                      betas=(opt.b1, opt.b2), eps=opt.eps, weight_decay=0)\n",
    "schedular = optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                      milestones = [50,200, 500, 1000, 2000], gamma = 0.5)\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"/home/zhizuo/lego/tensorboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the 0 epoch\n",
      "Average loss of epoch 1 : Loss 0.00330656\n",
      "starting the 1 epoch\n",
      "Average loss of epoch 2 : Loss 0.00328896\n",
      "starting the 2 epoch\n",
      "Average loss of epoch 3 : Loss 0.00328128\n",
      "starting the 3 epoch\n",
      "Average loss of epoch 4 : Loss 0.00325151\n",
      "starting the 4 epoch\n",
      "Average loss of epoch 5 : Loss 0.00323545\n",
      "starting the 5 epoch\n",
      "Average loss of epoch 6 : Loss 0.00320725\n",
      "starting the 6 epoch\n",
      "Average loss of epoch 7 : Loss 0.00317694\n",
      "starting the 7 epoch\n",
      "Average loss of epoch 8 : Loss 0.00314886\n",
      "starting the 8 epoch\n",
      "Average loss of epoch 9 : Loss 0.00311880\n",
      "starting the 9 epoch\n",
      "Average loss of epoch 10 : Loss 0.00308363\n",
      "starting the 10 epoch\n",
      "Average loss of epoch 11 : Loss 0.00304515\n",
      "starting the 11 epoch\n",
      "Average loss of epoch 12 : Loss 0.00301560\n",
      "starting the 12 epoch\n",
      "Average loss of epoch 13 : Loss 0.00298900\n",
      "starting the 13 epoch\n",
      "Average loss of epoch 14 : Loss 0.00295785\n",
      "starting the 14 epoch\n",
      "Average loss of epoch 15 : Loss 0.00293354\n",
      "starting the 15 epoch\n",
      "Average loss of epoch 16 : Loss 0.00290654\n",
      "starting the 16 epoch\n",
      "Average loss of epoch 17 : Loss 0.00289290\n",
      "starting the 17 epoch\n",
      "Average loss of epoch 18 : Loss 0.00286108\n",
      "starting the 18 epoch\n",
      "Average loss of epoch 19 : Loss 0.00283282\n",
      "starting the 19 epoch\n",
      "Average loss of epoch 20 : Loss 0.00279653\n",
      "starting the 20 epoch\n",
      "Average loss of epoch 21 : Loss 0.00278597\n",
      "starting the 21 epoch\n",
      "Average loss of epoch 22 : Loss 0.00276085\n",
      "starting the 22 epoch\n",
      "Average loss of epoch 23 : Loss 0.00272148\n",
      "starting the 23 epoch\n",
      "Average loss of epoch 24 : Loss 0.00270127\n",
      "starting the 24 epoch\n",
      "Average loss of epoch 25 : Loss 0.00267829\n",
      "starting the 25 epoch\n",
      "Average loss of epoch 26 : Loss 0.00265079\n",
      "starting the 26 epoch\n",
      "Average loss of epoch 27 : Loss 0.00262392\n",
      "starting the 27 epoch\n",
      "Average loss of epoch 28 : Loss 0.00259912\n",
      "starting the 28 epoch\n",
      "Average loss of epoch 29 : Loss 0.00257477\n",
      "starting the 29 epoch\n",
      "Average loss of epoch 30 : Loss 0.00256078\n",
      "starting the 30 epoch\n",
      "Average loss of epoch 31 : Loss 0.00253072\n",
      "starting the 31 epoch\n",
      "Average loss of epoch 32 : Loss 0.00252474\n",
      "starting the 32 epoch\n",
      "Average loss of epoch 33 : Loss 0.00249332\n",
      "starting the 33 epoch\n",
      "Average loss of epoch 34 : Loss 0.00246983\n",
      "starting the 34 epoch\n",
      "Average loss of epoch 35 : Loss 0.00244519\n",
      "starting the 35 epoch\n",
      "Average loss of epoch 36 : Loss 0.00243266\n",
      "starting the 36 epoch\n",
      "Average loss of epoch 37 : Loss 0.00240070\n",
      "starting the 37 epoch\n",
      "Average loss of epoch 38 : Loss 0.00237770\n",
      "starting the 38 epoch\n",
      "Average loss of epoch 39 : Loss 0.00235830\n",
      "starting the 39 epoch\n",
      "Average loss of epoch 40 : Loss 0.00234773\n",
      "starting the 40 epoch\n",
      "Average loss of epoch 41 : Loss 0.00233532\n",
      "starting the 41 epoch\n",
      "Average loss of epoch 42 : Loss 0.00229813\n",
      "starting the 42 epoch\n",
      "Average loss of epoch 43 : Loss 0.00228061\n",
      "starting the 43 epoch\n",
      "Average loss of epoch 44 : Loss 0.00226778\n",
      "starting the 44 epoch\n",
      "Average loss of epoch 45 : Loss 0.00223960\n",
      "starting the 45 epoch\n",
      "Average loss of epoch 46 : Loss 0.00220630\n",
      "starting the 46 epoch\n",
      "Average loss of epoch 47 : Loss 0.00219523\n",
      "starting the 47 epoch\n",
      "Average loss of epoch 48 : Loss 0.00217426\n",
      "starting the 48 epoch\n",
      "Average loss of epoch 49 : Loss 0.00215917\n",
      "starting the 49 epoch\n",
      "Average loss of epoch 50 : Loss 0.00212865\n",
      "starting the 50 epoch\n",
      "Average loss of epoch 51 : Loss 0.00211527\n",
      "starting the 51 epoch\n",
      "Average loss of epoch 52 : Loss 0.00210207\n",
      "starting the 52 epoch\n",
      "Average loss of epoch 53 : Loss 0.00207149\n",
      "starting the 53 epoch\n",
      "Average loss of epoch 54 : Loss 0.00205274\n",
      "starting the 54 epoch\n",
      "Average loss of epoch 55 : Loss 0.00203002\n",
      "starting the 55 epoch\n",
      "Average loss of epoch 56 : Loss 0.00202000\n",
      "starting the 56 epoch\n",
      "Average loss of epoch 57 : Loss 0.00199698\n",
      "starting the 57 epoch\n",
      "Average loss of epoch 58 : Loss 0.00197563\n",
      "starting the 58 epoch\n",
      "Average loss of epoch 59 : Loss 0.00196204\n",
      "starting the 59 epoch\n",
      "Average loss of epoch 60 : Loss 0.00193914\n",
      "starting the 60 epoch\n",
      "Average loss of epoch 61 : Loss 0.00191264\n",
      "starting the 61 epoch\n",
      "Average loss of epoch 62 : Loss 0.00189678\n",
      "starting the 62 epoch\n",
      "Average loss of epoch 63 : Loss 0.00186947\n",
      "starting the 63 epoch\n",
      "Average loss of epoch 64 : Loss 0.00184192\n",
      "starting the 64 epoch\n",
      "Average loss of epoch 65 : Loss 0.00182240\n",
      "starting the 65 epoch\n",
      "Average loss of epoch 66 : Loss 0.00180682\n",
      "starting the 66 epoch\n",
      "Average loss of epoch 67 : Loss 0.00178776\n",
      "starting the 67 epoch\n",
      "Average loss of epoch 68 : Loss 0.00176343\n",
      "starting the 68 epoch\n",
      "Average loss of epoch 69 : Loss 0.00174148\n",
      "starting the 69 epoch\n",
      "Average loss of epoch 70 : Loss 0.00172774\n",
      "starting the 70 epoch\n",
      "Average loss of epoch 71 : Loss 0.00171221\n",
      "starting the 71 epoch\n",
      "Average loss of epoch 72 : Loss 0.00170826\n",
      "starting the 72 epoch\n",
      "Average loss of epoch 73 : Loss 0.00166954\n",
      "starting the 73 epoch\n",
      "Average loss of epoch 74 : Loss 0.00165752\n",
      "starting the 74 epoch\n",
      "Average loss of epoch 75 : Loss 0.00163128\n",
      "starting the 75 epoch\n",
      "Average loss of epoch 76 : Loss 0.00160760\n",
      "starting the 76 epoch\n",
      "Average loss of epoch 77 : Loss 0.00158517\n",
      "starting the 77 epoch\n",
      "Average loss of epoch 78 : Loss 0.00156758\n",
      "starting the 78 epoch\n",
      "Average loss of epoch 79 : Loss 0.00154385\n",
      "starting the 79 epoch\n",
      "Average loss of epoch 80 : Loss 0.00152856\n",
      "starting the 80 epoch\n",
      "Average loss of epoch 81 : Loss 0.00150267\n",
      "starting the 81 epoch\n",
      "Average loss of epoch 82 : Loss 0.00147790\n",
      "starting the 82 epoch\n",
      "Average loss of epoch 83 : Loss 0.00146170\n",
      "starting the 83 epoch\n",
      "Average loss of epoch 84 : Loss 0.00145273\n",
      "starting the 84 epoch\n",
      "Average loss of epoch 85 : Loss 0.00143940\n",
      "starting the 85 epoch\n",
      "Average loss of epoch 86 : Loss 0.00141331\n",
      "starting the 86 epoch\n",
      "Average loss of epoch 87 : Loss 0.00139448\n",
      "starting the 87 epoch\n",
      "Average loss of epoch 88 : Loss 0.00138562\n",
      "starting the 88 epoch\n",
      "Average loss of epoch 89 : Loss 0.00136598\n",
      "starting the 89 epoch\n",
      "Average loss of epoch 90 : Loss 0.00134398\n",
      "starting the 90 epoch\n",
      "Average loss of epoch 91 : Loss 0.00132483\n",
      "starting the 91 epoch\n",
      "Average loss of epoch 92 : Loss 0.00130485\n",
      "starting the 92 epoch\n",
      "Average loss of epoch 93 : Loss 0.00127636\n",
      "starting the 93 epoch\n",
      "Average loss of epoch 94 : Loss 0.00124348\n",
      "starting the 94 epoch\n",
      "Average loss of epoch 95 : Loss 0.00123364\n",
      "starting the 95 epoch\n",
      "Average loss of epoch 96 : Loss 0.00121113\n",
      "starting the 96 epoch\n",
      "Average loss of epoch 97 : Loss 0.00119242\n",
      "starting the 97 epoch\n",
      "Average loss of epoch 98 : Loss 0.00117586\n",
      "starting the 98 epoch\n",
      "Average loss of epoch 99 : Loss 0.00116664\n",
      "starting the 99 epoch\n",
      "Average loss of epoch 100 : Loss 0.00115548\n",
      "starting the 100 epoch\n",
      "Average loss of epoch 101 : Loss 0.00113659\n",
      "starting the 101 epoch\n",
      "Average loss of epoch 102 : Loss 0.00111570\n",
      "starting the 102 epoch\n",
      "Average loss of epoch 103 : Loss 0.00109680\n",
      "starting the 103 epoch\n",
      "Average loss of epoch 104 : Loss 0.00108625\n",
      "starting the 104 epoch\n",
      "Average loss of epoch 105 : Loss 0.00106532\n",
      "starting the 105 epoch\n",
      "Average loss of epoch 106 : Loss 0.00105687\n",
      "starting the 106 epoch\n",
      "Average loss of epoch 107 : Loss 0.00103621\n",
      "starting the 107 epoch\n",
      "Average loss of epoch 108 : Loss 0.00102997\n",
      "starting the 108 epoch\n",
      "Average loss of epoch 109 : Loss 0.00101010\n",
      "starting the 109 epoch\n",
      "Average loss of epoch 110 : Loss 0.00098822\n",
      "starting the 110 epoch\n",
      "Average loss of epoch 111 : Loss 0.00097719\n",
      "starting the 111 epoch\n",
      "Average loss of epoch 112 : Loss 0.00095149\n",
      "starting the 112 epoch\n",
      "Average loss of epoch 113 : Loss 0.00094441\n",
      "starting the 113 epoch\n",
      "Average loss of epoch 114 : Loss 0.00093147\n",
      "starting the 114 epoch\n",
      "Average loss of epoch 115 : Loss 0.00092176\n",
      "starting the 115 epoch\n",
      "Average loss of epoch 116 : Loss 0.00089831\n",
      "starting the 116 epoch\n",
      "Average loss of epoch 117 : Loss 0.00088947\n",
      "starting the 117 epoch\n",
      "Average loss of epoch 118 : Loss 0.00087477\n",
      "starting the 118 epoch\n",
      "Average loss of epoch 119 : Loss 0.00086332\n",
      "starting the 119 epoch\n",
      "Average loss of epoch 120 : Loss 0.00085869\n",
      "starting the 120 epoch\n",
      "Average loss of epoch 121 : Loss 0.00084375\n",
      "starting the 121 epoch\n",
      "Average loss of epoch 122 : Loss 0.00083627\n",
      "starting the 122 epoch\n",
      "Average loss of epoch 123 : Loss 0.00082627\n",
      "starting the 123 epoch\n",
      "Average loss of epoch 124 : Loss 0.00081043\n",
      "starting the 124 epoch\n",
      "Average loss of epoch 125 : Loss 0.00080071\n",
      "starting the 125 epoch\n",
      "Average loss of epoch 126 : Loss 0.00079425\n",
      "starting the 126 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 127 : Loss 0.00078015\n",
      "starting the 127 epoch\n",
      "Average loss of epoch 128 : Loss 0.00076535\n",
      "starting the 128 epoch\n",
      "Average loss of epoch 129 : Loss 0.00075699\n",
      "starting the 129 epoch\n",
      "Average loss of epoch 130 : Loss 0.00074544\n",
      "starting the 130 epoch\n",
      "Average loss of epoch 131 : Loss 0.00072892\n",
      "starting the 131 epoch\n",
      "Average loss of epoch 132 : Loss 0.00071883\n",
      "starting the 132 epoch\n",
      "Average loss of epoch 133 : Loss 0.00072496\n",
      "starting the 133 epoch\n",
      "Average loss of epoch 134 : Loss 0.00072151\n",
      "starting the 134 epoch\n",
      "Average loss of epoch 135 : Loss 0.00070277\n",
      "starting the 135 epoch\n",
      "Average loss of epoch 136 : Loss 0.00068338\n",
      "starting the 136 epoch\n",
      "Average loss of epoch 137 : Loss 0.00067199\n",
      "starting the 137 epoch\n",
      "Average loss of epoch 138 : Loss 0.00065930\n",
      "starting the 138 epoch\n",
      "Average loss of epoch 139 : Loss 0.00065136\n",
      "starting the 139 epoch\n",
      "Average loss of epoch 140 : Loss 0.00064462\n",
      "starting the 140 epoch\n",
      "Average loss of epoch 141 : Loss 0.00063498\n",
      "starting the 141 epoch\n",
      "Average loss of epoch 142 : Loss 0.00063062\n",
      "starting the 142 epoch\n",
      "Average loss of epoch 143 : Loss 0.00062067\n",
      "starting the 143 epoch\n",
      "Average loss of epoch 144 : Loss 0.00062076\n",
      "starting the 144 epoch\n",
      "Average loss of epoch 145 : Loss 0.00062685\n",
      "starting the 145 epoch\n",
      "Average loss of epoch 146 : Loss 0.00061809\n",
      "starting the 146 epoch\n",
      "Average loss of epoch 147 : Loss 0.00061116\n",
      "starting the 147 epoch\n",
      "Average loss of epoch 148 : Loss 0.00059027\n",
      "starting the 148 epoch\n",
      "Average loss of epoch 149 : Loss 0.00057998\n",
      "starting the 149 epoch\n",
      "Average loss of epoch 150 : Loss 0.00057333\n",
      "starting the 150 epoch\n",
      "Average loss of epoch 151 : Loss 0.00056291\n",
      "starting the 151 epoch\n",
      "Average loss of epoch 152 : Loss 0.00056006\n",
      "starting the 152 epoch\n",
      "Average loss of epoch 153 : Loss 0.00055910\n",
      "starting the 153 epoch\n",
      "Average loss of epoch 154 : Loss 0.00054884\n",
      "starting the 154 epoch\n",
      "Average loss of epoch 155 : Loss 0.00054192\n",
      "starting the 155 epoch\n",
      "Average loss of epoch 156 : Loss 0.00053685\n",
      "starting the 156 epoch\n",
      "Average loss of epoch 157 : Loss 0.00053026\n",
      "starting the 157 epoch\n",
      "Average loss of epoch 158 : Loss 0.00052605\n",
      "starting the 158 epoch\n",
      "Average loss of epoch 159 : Loss 0.00052180\n",
      "starting the 159 epoch\n",
      "Average loss of epoch 160 : Loss 0.00051063\n",
      "starting the 160 epoch\n",
      "Average loss of epoch 161 : Loss 0.00049956\n",
      "starting the 161 epoch\n",
      "Average loss of epoch 162 : Loss 0.00049125\n",
      "starting the 162 epoch\n",
      "Average loss of epoch 163 : Loss 0.00048856\n",
      "starting the 163 epoch\n",
      "Average loss of epoch 164 : Loss 0.00048799\n",
      "starting the 164 epoch\n",
      "Average loss of epoch 165 : Loss 0.00049800\n",
      "starting the 165 epoch\n",
      "Average loss of epoch 166 : Loss 0.00049187\n",
      "starting the 166 epoch\n",
      "Average loss of epoch 167 : Loss 0.00049067\n",
      "starting the 167 epoch\n",
      "Average loss of epoch 168 : Loss 0.00047419\n",
      "starting the 168 epoch\n",
      "Average loss of epoch 169 : Loss 0.00045567\n",
      "starting the 169 epoch\n",
      "Average loss of epoch 170 : Loss 0.00045084\n",
      "starting the 170 epoch\n",
      "Average loss of epoch 171 : Loss 0.00044916\n",
      "starting the 171 epoch\n",
      "Average loss of epoch 172 : Loss 0.00043888\n",
      "starting the 172 epoch\n",
      "Average loss of epoch 173 : Loss 0.00043569\n",
      "starting the 173 epoch\n",
      "Average loss of epoch 174 : Loss 0.00043039\n",
      "starting the 174 epoch\n",
      "Average loss of epoch 175 : Loss 0.00042484\n",
      "starting the 175 epoch\n",
      "Average loss of epoch 176 : Loss 0.00041920\n",
      "starting the 176 epoch\n",
      "Average loss of epoch 177 : Loss 0.00041962\n",
      "starting the 177 epoch\n",
      "Average loss of epoch 178 : Loss 0.00041921\n",
      "starting the 178 epoch\n",
      "Average loss of epoch 179 : Loss 0.00042577\n",
      "starting the 179 epoch\n",
      "Average loss of epoch 180 : Loss 0.00043350\n",
      "starting the 180 epoch\n",
      "Average loss of epoch 181 : Loss 0.00042245\n",
      "starting the 181 epoch\n",
      "Average loss of epoch 182 : Loss 0.00041471\n",
      "starting the 182 epoch\n",
      "Average loss of epoch 183 : Loss 0.00040146\n",
      "starting the 183 epoch\n",
      "Average loss of epoch 184 : Loss 0.00039069\n",
      "starting the 184 epoch\n",
      "Average loss of epoch 185 : Loss 0.00038380\n",
      "starting the 185 epoch\n",
      "Average loss of epoch 186 : Loss 0.00037828\n",
      "starting the 186 epoch\n",
      "Average loss of epoch 187 : Loss 0.00037347\n",
      "starting the 187 epoch\n",
      "Average loss of epoch 188 : Loss 0.00037071\n",
      "starting the 188 epoch\n",
      "Average loss of epoch 189 : Loss 0.00036897\n",
      "starting the 189 epoch\n",
      "Average loss of epoch 190 : Loss 0.00036423\n",
      "starting the 190 epoch\n",
      "Average loss of epoch 191 : Loss 0.00036330\n",
      "starting the 191 epoch\n",
      "Average loss of epoch 192 : Loss 0.00036290\n",
      "starting the 192 epoch\n",
      "Average loss of epoch 193 : Loss 0.00036625\n",
      "starting the 193 epoch\n",
      "Average loss of epoch 194 : Loss 0.00036462\n",
      "starting the 194 epoch\n",
      "Average loss of epoch 195 : Loss 0.00035818\n",
      "starting the 195 epoch\n",
      "Average loss of epoch 196 : Loss 0.00035698\n",
      "starting the 196 epoch\n",
      "Average loss of epoch 197 : Loss 0.00035188\n",
      "starting the 197 epoch\n",
      "Average loss of epoch 198 : Loss 0.00034410\n",
      "starting the 198 epoch\n",
      "Average loss of epoch 199 : Loss 0.00033945\n",
      "starting the 199 epoch\n",
      "Average loss of epoch 200 : Loss 0.00033115\n",
      "starting the 200 epoch\n",
      "Average loss of epoch 201 : Loss 0.00032810\n",
      "starting the 201 epoch\n",
      "Average loss of epoch 202 : Loss 0.00033977\n",
      "starting the 202 epoch\n",
      "Average loss of epoch 203 : Loss 0.00034429\n",
      "starting the 203 epoch\n",
      "Average loss of epoch 204 : Loss 0.00033760\n",
      "starting the 204 epoch\n",
      "Average loss of epoch 205 : Loss 0.00033795\n",
      "starting the 205 epoch\n",
      "Average loss of epoch 206 : Loss 0.00032752\n",
      "starting the 206 epoch\n",
      "Average loss of epoch 207 : Loss 0.00031935\n",
      "starting the 207 epoch\n",
      "Average loss of epoch 208 : Loss 0.00031045\n",
      "starting the 208 epoch\n",
      "Average loss of epoch 209 : Loss 0.00030713\n",
      "starting the 209 epoch\n",
      "Average loss of epoch 210 : Loss 0.00031457\n",
      "starting the 210 epoch\n",
      "Average loss of epoch 211 : Loss 0.00031124\n",
      "starting the 211 epoch\n",
      "Average loss of epoch 212 : Loss 0.00030362\n",
      "starting the 212 epoch\n",
      "Average loss of epoch 213 : Loss 0.00029596\n",
      "starting the 213 epoch\n",
      "Average loss of epoch 214 : Loss 0.00029065\n",
      "starting the 214 epoch\n",
      "Average loss of epoch 215 : Loss 0.00028894\n",
      "starting the 215 epoch\n",
      "Average loss of epoch 216 : Loss 0.00028932\n",
      "starting the 216 epoch\n",
      "Average loss of epoch 217 : Loss 0.00029062\n",
      "starting the 217 epoch\n",
      "Average loss of epoch 218 : Loss 0.00028593\n",
      "starting the 218 epoch\n",
      "Average loss of epoch 219 : Loss 0.00028582\n",
      "starting the 219 epoch\n",
      "Average loss of epoch 220 : Loss 0.00028190\n",
      "starting the 220 epoch\n",
      "Average loss of epoch 221 : Loss 0.00028420\n",
      "starting the 221 epoch\n",
      "Average loss of epoch 222 : Loss 0.00028426\n",
      "starting the 222 epoch\n",
      "Average loss of epoch 223 : Loss 0.00028800\n",
      "starting the 223 epoch\n",
      "Average loss of epoch 224 : Loss 0.00027832\n",
      "starting the 224 epoch\n",
      "Average loss of epoch 225 : Loss 0.00027325\n",
      "starting the 225 epoch\n",
      "Average loss of epoch 226 : Loss 0.00026429\n",
      "starting the 226 epoch\n",
      "Average loss of epoch 227 : Loss 0.00025951\n",
      "starting the 227 epoch\n",
      "Average loss of epoch 228 : Loss 0.00026070\n",
      "starting the 228 epoch\n",
      "Average loss of epoch 229 : Loss 0.00026244\n",
      "starting the 229 epoch\n",
      "Average loss of epoch 230 : Loss 0.00026500\n",
      "starting the 230 epoch\n",
      "Average loss of epoch 231 : Loss 0.00026199\n",
      "starting the 231 epoch\n",
      "Average loss of epoch 232 : Loss 0.00026019\n",
      "starting the 232 epoch\n",
      "Average loss of epoch 233 : Loss 0.00025575\n",
      "starting the 233 epoch\n",
      "Average loss of epoch 234 : Loss 0.00024914\n",
      "starting the 234 epoch\n",
      "Average loss of epoch 235 : Loss 0.00024568\n",
      "starting the 235 epoch\n",
      "Average loss of epoch 236 : Loss 0.00024570\n",
      "starting the 236 epoch\n",
      "Average loss of epoch 237 : Loss 0.00024434\n",
      "starting the 237 epoch\n",
      "Average loss of epoch 238 : Loss 0.00024393\n",
      "starting the 238 epoch\n",
      "Average loss of epoch 239 : Loss 0.00024005\n",
      "starting the 239 epoch\n",
      "Average loss of epoch 240 : Loss 0.00023891\n",
      "starting the 240 epoch\n",
      "Average loss of epoch 241 : Loss 0.00023712\n",
      "starting the 241 epoch\n",
      "Average loss of epoch 242 : Loss 0.00023545\n",
      "starting the 242 epoch\n",
      "Average loss of epoch 243 : Loss 0.00023574\n",
      "starting the 243 epoch\n",
      "Average loss of epoch 244 : Loss 0.00023524\n",
      "starting the 244 epoch\n",
      "Average loss of epoch 245 : Loss 0.00023244\n",
      "starting the 245 epoch\n",
      "Average loss of epoch 246 : Loss 0.00023919\n",
      "starting the 246 epoch\n",
      "Average loss of epoch 247 : Loss 0.00023306\n",
      "starting the 247 epoch\n",
      "Average loss of epoch 248 : Loss 0.00022957\n",
      "starting the 248 epoch\n",
      "Average loss of epoch 249 : Loss 0.00022775\n",
      "starting the 249 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 250 : Loss 0.00022559\n",
      "starting the 250 epoch\n",
      "Average loss of epoch 251 : Loss 0.00022250\n",
      "starting the 251 epoch\n",
      "Average loss of epoch 252 : Loss 0.00021996\n",
      "starting the 252 epoch\n",
      "Average loss of epoch 253 : Loss 0.00021463\n",
      "starting the 253 epoch\n",
      "Average loss of epoch 254 : Loss 0.00021958\n",
      "starting the 254 epoch\n",
      "Average loss of epoch 255 : Loss 0.00021348\n",
      "starting the 255 epoch\n",
      "Average loss of epoch 256 : Loss 0.00021018\n",
      "starting the 256 epoch\n",
      "Average loss of epoch 257 : Loss 0.00020484\n",
      "starting the 257 epoch\n",
      "Average loss of epoch 258 : Loss 0.00020521\n",
      "starting the 258 epoch\n",
      "Average loss of epoch 259 : Loss 0.00020143\n",
      "starting the 259 epoch\n",
      "Average loss of epoch 260 : Loss 0.00020411\n",
      "starting the 260 epoch\n",
      "Average loss of epoch 261 : Loss 0.00020302\n",
      "starting the 261 epoch\n",
      "Average loss of epoch 262 : Loss 0.00020098\n",
      "starting the 262 epoch\n",
      "Average loss of epoch 263 : Loss 0.00019909\n",
      "starting the 263 epoch\n",
      "Average loss of epoch 264 : Loss 0.00020281\n",
      "starting the 264 epoch\n",
      "Average loss of epoch 265 : Loss 0.00020309\n",
      "starting the 265 epoch\n",
      "Average loss of epoch 266 : Loss 0.00020843\n",
      "starting the 266 epoch\n",
      "Average loss of epoch 267 : Loss 0.00020889\n",
      "starting the 267 epoch\n",
      "Average loss of epoch 268 : Loss 0.00020882\n",
      "starting the 268 epoch\n",
      "Average loss of epoch 269 : Loss 0.00020015\n",
      "starting the 269 epoch\n",
      "Average loss of epoch 270 : Loss 0.00019811\n",
      "starting the 270 epoch\n",
      "Average loss of epoch 271 : Loss 0.00019466\n",
      "starting the 271 epoch\n",
      "Average loss of epoch 272 : Loss 0.00019338\n",
      "starting the 272 epoch\n",
      "Average loss of epoch 273 : Loss 0.00018549\n",
      "starting the 273 epoch\n",
      "Average loss of epoch 274 : Loss 0.00018297\n",
      "starting the 274 epoch\n",
      "Average loss of epoch 275 : Loss 0.00018195\n",
      "starting the 275 epoch\n",
      "Average loss of epoch 276 : Loss 0.00018015\n",
      "starting the 276 epoch\n",
      "Average loss of epoch 277 : Loss 0.00018127\n",
      "starting the 277 epoch\n",
      "Average loss of epoch 278 : Loss 0.00018069\n",
      "starting the 278 epoch\n",
      "Average loss of epoch 279 : Loss 0.00017863\n",
      "starting the 279 epoch\n",
      "Average loss of epoch 280 : Loss 0.00018501\n",
      "starting the 280 epoch\n",
      "Average loss of epoch 281 : Loss 0.00018711\n",
      "starting the 281 epoch\n",
      "Average loss of epoch 282 : Loss 0.00019118\n",
      "starting the 282 epoch\n",
      "Average loss of epoch 283 : Loss 0.00018760\n",
      "starting the 283 epoch\n",
      "Average loss of epoch 284 : Loss 0.00018093\n",
      "starting the 284 epoch\n",
      "Average loss of epoch 285 : Loss 0.00017825\n",
      "starting the 285 epoch\n",
      "Average loss of epoch 286 : Loss 0.00017758\n",
      "starting the 286 epoch\n",
      "Average loss of epoch 287 : Loss 0.00017358\n",
      "starting the 287 epoch\n",
      "Average loss of epoch 288 : Loss 0.00017297\n",
      "starting the 288 epoch\n",
      "Average loss of epoch 289 : Loss 0.00016809\n",
      "starting the 289 epoch\n",
      "Average loss of epoch 290 : Loss 0.00016189\n",
      "starting the 290 epoch\n",
      "Average loss of epoch 291 : Loss 0.00015945\n",
      "starting the 291 epoch\n",
      "Average loss of epoch 292 : Loss 0.00015941\n",
      "starting the 292 epoch\n",
      "Average loss of epoch 293 : Loss 0.00016295\n",
      "starting the 293 epoch\n",
      "Average loss of epoch 294 : Loss 0.00016165\n",
      "starting the 294 epoch\n",
      "Average loss of epoch 295 : Loss 0.00016479\n",
      "starting the 295 epoch\n",
      "Average loss of epoch 296 : Loss 0.00016327\n",
      "starting the 296 epoch\n",
      "Average loss of epoch 297 : Loss 0.00016284\n",
      "starting the 297 epoch\n",
      "Average loss of epoch 298 : Loss 0.00016137\n",
      "starting the 298 epoch\n",
      "Average loss of epoch 299 : Loss 0.00016020\n",
      "starting the 299 epoch\n",
      "Average loss of epoch 300 : Loss 0.00015696\n",
      "starting the 300 epoch\n",
      "Average loss of epoch 301 : Loss 0.00015585\n",
      "starting the 301 epoch\n",
      "Average loss of epoch 302 : Loss 0.00015619\n",
      "starting the 302 epoch\n",
      "Average loss of epoch 303 : Loss 0.00015748\n",
      "starting the 303 epoch\n",
      "Average loss of epoch 304 : Loss 0.00015805\n",
      "starting the 304 epoch\n",
      "Average loss of epoch 305 : Loss 0.00015605\n",
      "starting the 305 epoch\n",
      "Average loss of epoch 306 : Loss 0.00015776\n",
      "starting the 306 epoch\n",
      "Average loss of epoch 307 : Loss 0.00015548\n",
      "starting the 307 epoch\n",
      "Average loss of epoch 308 : Loss 0.00015578\n",
      "starting the 308 epoch\n",
      "Average loss of epoch 309 : Loss 0.00015574\n",
      "starting the 309 epoch\n",
      "Average loss of epoch 310 : Loss 0.00015614\n",
      "starting the 310 epoch\n",
      "Average loss of epoch 311 : Loss 0.00015805\n",
      "starting the 311 epoch\n",
      "Average loss of epoch 312 : Loss 0.00015585\n",
      "starting the 312 epoch\n",
      "Average loss of epoch 313 : Loss 0.00014804\n",
      "starting the 313 epoch\n",
      "Average loss of epoch 314 : Loss 0.00014232\n",
      "starting the 314 epoch\n",
      "Average loss of epoch 315 : Loss 0.00014364\n",
      "starting the 315 epoch\n",
      "Average loss of epoch 316 : Loss 0.00014474\n",
      "starting the 316 epoch\n",
      "Average loss of epoch 317 : Loss 0.00014492\n",
      "starting the 317 epoch\n",
      "Average loss of epoch 318 : Loss 0.00014909\n",
      "starting the 318 epoch\n",
      "Average loss of epoch 319 : Loss 0.00015260\n",
      "starting the 319 epoch\n",
      "Average loss of epoch 320 : Loss 0.00014910\n",
      "starting the 320 epoch\n",
      "Average loss of epoch 321 : Loss 0.00014443\n",
      "starting the 321 epoch\n",
      "Average loss of epoch 322 : Loss 0.00014121\n",
      "starting the 322 epoch\n",
      "Average loss of epoch 323 : Loss 0.00013775\n",
      "starting the 323 epoch\n",
      "Average loss of epoch 324 : Loss 0.00013783\n",
      "starting the 324 epoch\n",
      "Average loss of epoch 325 : Loss 0.00013711\n",
      "starting the 325 epoch\n",
      "Average loss of epoch 326 : Loss 0.00013368\n",
      "starting the 326 epoch\n",
      "Average loss of epoch 327 : Loss 0.00013292\n",
      "starting the 327 epoch\n",
      "Average loss of epoch 328 : Loss 0.00013169\n",
      "starting the 328 epoch\n",
      "Average loss of epoch 329 : Loss 0.00012932\n",
      "starting the 329 epoch\n",
      "Average loss of epoch 330 : Loss 0.00012919\n",
      "starting the 330 epoch\n",
      "Average loss of epoch 331 : Loss 0.00012680\n",
      "starting the 331 epoch\n",
      "Average loss of epoch 332 : Loss 0.00012668\n",
      "starting the 332 epoch\n",
      "Average loss of epoch 333 : Loss 0.00012728\n",
      "starting the 333 epoch\n",
      "Average loss of epoch 334 : Loss 0.00012982\n",
      "starting the 334 epoch\n",
      "Average loss of epoch 335 : Loss 0.00013116\n",
      "starting the 335 epoch\n",
      "Average loss of epoch 336 : Loss 0.00013173\n",
      "starting the 336 epoch\n",
      "Average loss of epoch 337 : Loss 0.00012982\n",
      "starting the 337 epoch\n",
      "Average loss of epoch 338 : Loss 0.00013269\n",
      "starting the 338 epoch\n",
      "Average loss of epoch 339 : Loss 0.00013118\n",
      "starting the 339 epoch\n",
      "Average loss of epoch 340 : Loss 0.00012872\n",
      "starting the 340 epoch\n",
      "Average loss of epoch 341 : Loss 0.00012515\n",
      "starting the 341 epoch\n",
      "Average loss of epoch 342 : Loss 0.00012283\n",
      "starting the 342 epoch\n",
      "Average loss of epoch 343 : Loss 0.00012089\n",
      "starting the 343 epoch\n",
      "Average loss of epoch 344 : Loss 0.00012116\n",
      "starting the 344 epoch\n",
      "Average loss of epoch 345 : Loss 0.00011879\n",
      "starting the 345 epoch\n",
      "Average loss of epoch 346 : Loss 0.00011929\n",
      "starting the 346 epoch\n",
      "Average loss of epoch 347 : Loss 0.00012131\n",
      "starting the 347 epoch\n",
      "Average loss of epoch 348 : Loss 0.00012164\n",
      "starting the 348 epoch\n",
      "Average loss of epoch 349 : Loss 0.00012574\n",
      "starting the 349 epoch\n",
      "Average loss of epoch 350 : Loss 0.00012973\n",
      "starting the 350 epoch\n",
      "Average loss of epoch 351 : Loss 0.00012804\n",
      "starting the 351 epoch\n",
      "Average loss of epoch 352 : Loss 0.00012367\n",
      "starting the 352 epoch\n",
      "Average loss of epoch 353 : Loss 0.00012360\n",
      "starting the 353 epoch\n",
      "Average loss of epoch 354 : Loss 0.00011988\n",
      "starting the 354 epoch\n",
      "Average loss of epoch 355 : Loss 0.00011636\n",
      "starting the 355 epoch\n",
      "Average loss of epoch 356 : Loss 0.00011569\n",
      "starting the 356 epoch\n",
      "Average loss of epoch 357 : Loss 0.00011872\n",
      "starting the 357 epoch\n",
      "Average loss of epoch 358 : Loss 0.00011722\n",
      "starting the 358 epoch\n",
      "Average loss of epoch 359 : Loss 0.00011645\n",
      "starting the 359 epoch\n",
      "Average loss of epoch 360 : Loss 0.00011528\n",
      "starting the 360 epoch\n",
      "Average loss of epoch 361 : Loss 0.00012218\n",
      "starting the 361 epoch\n",
      "Average loss of epoch 362 : Loss 0.00012557\n",
      "starting the 362 epoch\n",
      "Average loss of epoch 363 : Loss 0.00012280\n",
      "starting the 363 epoch\n",
      "Average loss of epoch 364 : Loss 0.00011964\n",
      "starting the 364 epoch\n",
      "Average loss of epoch 365 : Loss 0.00011413\n",
      "starting the 365 epoch\n",
      "Average loss of epoch 366 : Loss 0.00010799\n",
      "starting the 366 epoch\n",
      "Average loss of epoch 367 : Loss 0.00010417\n",
      "starting the 367 epoch\n",
      "Average loss of epoch 368 : Loss 0.00010394\n",
      "starting the 368 epoch\n",
      "Average loss of epoch 369 : Loss 0.00010152\n",
      "starting the 369 epoch\n",
      "Average loss of epoch 370 : Loss 0.00010191\n",
      "starting the 370 epoch\n",
      "Average loss of epoch 371 : Loss 0.00010518\n",
      "starting the 371 epoch\n",
      "Average loss of epoch 372 : Loss 0.00010679\n",
      "starting the 372 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 373 : Loss 0.00010578\n",
      "starting the 373 epoch\n",
      "Average loss of epoch 374 : Loss 0.00010522\n",
      "starting the 374 epoch\n",
      "Average loss of epoch 375 : Loss 0.00010433\n",
      "starting the 375 epoch\n",
      "Average loss of epoch 376 : Loss 0.00010593\n",
      "starting the 376 epoch\n",
      "Average loss of epoch 377 : Loss 0.00010531\n",
      "starting the 377 epoch\n",
      "Average loss of epoch 378 : Loss 0.00010451\n",
      "starting the 378 epoch\n",
      "Average loss of epoch 379 : Loss 0.00010663\n",
      "starting the 379 epoch\n",
      "Average loss of epoch 380 : Loss 0.00010629\n",
      "starting the 380 epoch\n",
      "Average loss of epoch 381 : Loss 0.00010568\n",
      "starting the 381 epoch\n",
      "Average loss of epoch 382 : Loss 0.00010498\n",
      "starting the 382 epoch\n",
      "Average loss of epoch 383 : Loss 0.00010454\n",
      "starting the 383 epoch\n",
      "Average loss of epoch 384 : Loss 0.00010623\n",
      "starting the 384 epoch\n",
      "Average loss of epoch 385 : Loss 0.00010522\n",
      "starting the 385 epoch\n",
      "Average loss of epoch 386 : Loss 0.00010356\n",
      "starting the 386 epoch\n",
      "Average loss of epoch 387 : Loss 0.00010166\n",
      "starting the 387 epoch\n",
      "Average loss of epoch 388 : Loss 0.00010181\n",
      "starting the 388 epoch\n",
      "Average loss of epoch 389 : Loss 0.00010008\n",
      "starting the 389 epoch\n",
      "Average loss of epoch 390 : Loss 0.00009827\n",
      "starting the 390 epoch\n",
      "Average loss of epoch 391 : Loss 0.00009980\n",
      "starting the 391 epoch\n",
      "Average loss of epoch 392 : Loss 0.00010197\n",
      "starting the 392 epoch\n",
      "Average loss of epoch 393 : Loss 0.00010413\n",
      "starting the 393 epoch\n",
      "Average loss of epoch 394 : Loss 0.00010616\n",
      "starting the 394 epoch\n",
      "Average loss of epoch 395 : Loss 0.00010371\n",
      "starting the 395 epoch\n",
      "Average loss of epoch 396 : Loss 0.00010214\n",
      "starting the 396 epoch\n",
      "Average loss of epoch 397 : Loss 0.00009982\n",
      "starting the 397 epoch\n",
      "Average loss of epoch 398 : Loss 0.00009763\n",
      "starting the 398 epoch\n",
      "Average loss of epoch 399 : Loss 0.00009410\n",
      "starting the 399 epoch\n",
      "Average loss of epoch 400 : Loss 0.00009387\n",
      "starting the 400 epoch\n",
      "Average loss of epoch 401 : Loss 0.00009158\n",
      "starting the 401 epoch\n",
      "Average loss of epoch 402 : Loss 0.00009022\n",
      "starting the 402 epoch\n",
      "Average loss of epoch 403 : Loss 0.00009270\n",
      "starting the 403 epoch\n",
      "Average loss of epoch 404 : Loss 0.00009366\n",
      "starting the 404 epoch\n",
      "Average loss of epoch 405 : Loss 0.00009235\n",
      "starting the 405 epoch\n",
      "Average loss of epoch 406 : Loss 0.00009296\n",
      "starting the 406 epoch\n",
      "Average loss of epoch 407 : Loss 0.00009474\n",
      "starting the 407 epoch\n",
      "Average loss of epoch 408 : Loss 0.00009454\n",
      "starting the 408 epoch\n",
      "Average loss of epoch 409 : Loss 0.00009472\n",
      "starting the 409 epoch\n",
      "Average loss of epoch 410 : Loss 0.00009466\n",
      "starting the 410 epoch\n",
      "Average loss of epoch 411 : Loss 0.00009252\n",
      "starting the 411 epoch\n",
      "Average loss of epoch 412 : Loss 0.00009042\n",
      "starting the 412 epoch\n",
      "Average loss of epoch 413 : Loss 0.00009143\n",
      "starting the 413 epoch\n",
      "Average loss of epoch 414 : Loss 0.00008972\n",
      "starting the 414 epoch\n",
      "Average loss of epoch 415 : Loss 0.00009026\n",
      "starting the 415 epoch\n",
      "Average loss of epoch 416 : Loss 0.00008812\n",
      "starting the 416 epoch\n",
      "Average loss of epoch 417 : Loss 0.00008844\n",
      "starting the 417 epoch\n",
      "Average loss of epoch 418 : Loss 0.00008676\n",
      "starting the 418 epoch\n",
      "Average loss of epoch 419 : Loss 0.00008868\n",
      "starting the 419 epoch\n",
      "Average loss of epoch 420 : Loss 0.00008836\n",
      "starting the 420 epoch\n",
      "Average loss of epoch 421 : Loss 0.00008999\n",
      "starting the 421 epoch\n",
      "Average loss of epoch 422 : Loss 0.00008970\n",
      "starting the 422 epoch\n",
      "Average loss of epoch 423 : Loss 0.00009205\n",
      "starting the 423 epoch\n",
      "Average loss of epoch 424 : Loss 0.00009231\n",
      "starting the 424 epoch\n",
      "Average loss of epoch 425 : Loss 0.00009280\n",
      "starting the 425 epoch\n",
      "Average loss of epoch 426 : Loss 0.00008911\n",
      "starting the 426 epoch\n",
      "Average loss of epoch 427 : Loss 0.00008906\n",
      "starting the 427 epoch\n",
      "Average loss of epoch 428 : Loss 0.00008707\n",
      "starting the 428 epoch\n",
      "Average loss of epoch 429 : Loss 0.00008778\n",
      "starting the 429 epoch\n",
      "Average loss of epoch 430 : Loss 0.00008503\n",
      "starting the 430 epoch\n",
      "Average loss of epoch 431 : Loss 0.00008517\n",
      "starting the 431 epoch\n",
      "Average loss of epoch 432 : Loss 0.00008706\n",
      "starting the 432 epoch\n",
      "Average loss of epoch 433 : Loss 0.00008450\n",
      "starting the 433 epoch\n",
      "Average loss of epoch 434 : Loss 0.00008302\n",
      "starting the 434 epoch\n",
      "Average loss of epoch 435 : Loss 0.00008366\n",
      "starting the 435 epoch\n",
      "Average loss of epoch 436 : Loss 0.00008146\n",
      "starting the 436 epoch\n",
      "Average loss of epoch 437 : Loss 0.00008145\n",
      "starting the 437 epoch\n",
      "Average loss of epoch 438 : Loss 0.00008218\n",
      "starting the 438 epoch\n",
      "Average loss of epoch 439 : Loss 0.00008146\n",
      "starting the 439 epoch\n",
      "Average loss of epoch 440 : Loss 0.00008198\n",
      "starting the 440 epoch\n",
      "Average loss of epoch 441 : Loss 0.00008049\n",
      "starting the 441 epoch\n",
      "Average loss of epoch 442 : Loss 0.00008095\n",
      "starting the 442 epoch\n",
      "Average loss of epoch 443 : Loss 0.00008173\n",
      "starting the 443 epoch\n",
      "Average loss of epoch 444 : Loss 0.00008120\n",
      "starting the 444 epoch\n",
      "Average loss of epoch 445 : Loss 0.00008078\n",
      "starting the 445 epoch\n",
      "Average loss of epoch 446 : Loss 0.00007932\n",
      "starting the 446 epoch\n",
      "Average loss of epoch 447 : Loss 0.00007793\n",
      "starting the 447 epoch\n",
      "Average loss of epoch 448 : Loss 0.00008092\n",
      "starting the 448 epoch\n",
      "Average loss of epoch 449 : Loss 0.00008009\n",
      "starting the 449 epoch\n",
      "Average loss of epoch 450 : Loss 0.00008047\n",
      "starting the 450 epoch\n",
      "Average loss of epoch 451 : Loss 0.00007954\n",
      "starting the 451 epoch\n",
      "Average loss of epoch 452 : Loss 0.00007890\n",
      "starting the 452 epoch\n",
      "Average loss of epoch 453 : Loss 0.00007707\n",
      "starting the 453 epoch\n",
      "Average loss of epoch 454 : Loss 0.00007799\n",
      "starting the 454 epoch\n",
      "Average loss of epoch 455 : Loss 0.00007787\n",
      "starting the 455 epoch\n",
      "Average loss of epoch 456 : Loss 0.00007747\n",
      "starting the 456 epoch\n",
      "Average loss of epoch 457 : Loss 0.00007890\n",
      "starting the 457 epoch\n",
      "Average loss of epoch 458 : Loss 0.00008024\n",
      "starting the 458 epoch\n",
      "Average loss of epoch 459 : Loss 0.00008294\n",
      "starting the 459 epoch\n",
      "Average loss of epoch 460 : Loss 0.00008390\n",
      "starting the 460 epoch\n",
      "Average loss of epoch 461 : Loss 0.00008092\n",
      "starting the 461 epoch\n",
      "Average loss of epoch 462 : Loss 0.00007633\n",
      "starting the 462 epoch\n",
      "Average loss of epoch 463 : Loss 0.00007362\n",
      "starting the 463 epoch\n",
      "Average loss of epoch 464 : Loss 0.00007207\n",
      "starting the 464 epoch\n",
      "Average loss of epoch 465 : Loss 0.00007156\n",
      "starting the 465 epoch\n",
      "Average loss of epoch 466 : Loss 0.00007160\n",
      "starting the 466 epoch\n",
      "Average loss of epoch 467 : Loss 0.00007174\n",
      "starting the 467 epoch\n",
      "Average loss of epoch 468 : Loss 0.00007220\n",
      "starting the 468 epoch\n",
      "Average loss of epoch 469 : Loss 0.00007111\n",
      "starting the 469 epoch\n",
      "Average loss of epoch 470 : Loss 0.00006991\n",
      "starting the 470 epoch\n",
      "Average loss of epoch 471 : Loss 0.00007143\n",
      "starting the 471 epoch\n"
     ]
    }
   ],
   "source": [
    "opt.max_epochs = 500\n",
    "epoch_loss = torch.zeros(opt.max_epochs)\n",
    "for epoch in range(opt.max_epochs):\n",
    "    print(\"starting the {} epoch\".format(epoch))\n",
    "    for training_data, training_label in train_loader:\n",
    "        if (torch.cuda.is_available()):\n",
    "            training_data = training_data.cuda()\n",
    "            training_label = training_label.cuda()\n",
    "        predict_label = net(training_data)\n",
    "        training_label = training_label.float()\n",
    "        predict_label = predict_label.view(predict_label.shape[0], predict_label.shape[2], -1)\n",
    "        #print(training_data.shape)\n",
    "        #print(predict_label.shape)\n",
    "        #print(training_label.shape)\n",
    "        loss = criterion(predict_label, training_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('Train Loss', loss.item(), epoch)\n",
    "        writer.flush()\n",
    "        epoch_loss[epoch] += loss\n",
    "    epoch_loss[epoch] = epoch_loss[epoch] / i\n",
    "    print(\"Average loss of epoch {} : Loss {:.8f}\".\n",
    "          format(epoch + 1, epoch_loss[epoch]))\n",
    "        \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\"Saving the Model\")\n",
    "        localtime = time.localtime(time.time())\n",
    "        torch.save(net.state_dict(), \n",
    "                  '/home/zhizuo/lego/models/FCN32s_{}_{}_{}_{}.pkl'.\n",
    "                  format(localtime[0], localtime[1], localtime[2], \n",
    "                         localtime[3], localtime[4]))\n",
    "        print(\"Successfully save the model of {}\".format(localtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, opt.max_epochs)\n",
    "\n",
    "plt.figure()\n",
    "l = plt.plot(x, epoch_loss, label = 'l2 loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
