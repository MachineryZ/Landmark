{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FCN to detect landmark point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the data\n",
    "import networkx as nx\n",
    "from process_it import process_it\n",
    "import os\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import math        \n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import csv\n",
    "from random import randint, random, seed\n",
    "import time\n",
    "\n",
    "def state_to_numpy(state):\n",
    "    strlist = state.split()\n",
    "    val_list = [float(s) for s in strlist]\n",
    "    return np.array(val_list)\n",
    "\n",
    "def numpy_to_state(array):\n",
    "    state = \"\"\n",
    "    for i in range(len(array)):\n",
    "        state += str(array[i])+\" \"\n",
    "    return state\n",
    "\n",
    "def list_all_dir(data_dir):\n",
    "    task_dirs = os.listdir(data_dir)\n",
    "\n",
    "    list_dir = []\n",
    "    for task_dir in task_dirs:\n",
    "        env_dirs = os.listdir(data_dir+\"/\"+task_dir)\n",
    "        for env_dir in env_dirs:\n",
    "            list_dir.append(data_dir +\"/\"+ task_dir +\"/\"+ env_dir)\n",
    "    return list_dir  \n",
    "\n",
    "def detection_data(G, directory, grid, with_start_goal=False):\n",
    "    start = np.loadtxt(directory+\"/start_nodes.txt\")\n",
    "    goal = np.loadtxt(directory+\"/goal_nodes.txt\")\n",
    "    occ_grid = np.loadtxt(directory+\"/occ_grid.txt\")\n",
    "    path_nodes = []\n",
    "    occ = {} # dict\n",
    "    ini = {}\n",
    "    end = {}\n",
    "    i = 0\n",
    "    all_data = []\n",
    "    with open(directory + \"/path_nodes.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip('\\n')\n",
    "            \n",
    "            s = state_to_numpy(G.nodes[str(int(start[i]))]['state'])\n",
    "            g = state_to_numpy(G.nodes[str(int(goal[i]))]['state'])\n",
    "            og = occ_grid[i]\n",
    "            path_nodes = str(line).split(\",\")\n",
    "            # print(path_nodes)\n",
    "            for path_node in path_nodes:\n",
    "                if(path_node=='-1'):\n",
    "                    continue\n",
    "                \n",
    "                node_conf = state_to_numpy(G.nodes[path_node]['state'])\n",
    "                curr_node = np.array([])\n",
    "                \n",
    "                og = og.astype(np.int32)\n",
    "                str_og = numpy_to_state(og)\n",
    "                if str_og not in occ.keys():\n",
    "                    occ[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                if str_og not in ini.keys():\n",
    "                    ini[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                if str_og not in end.keys():\n",
    "                    end[str_og] = np.zeros((10*grid, 10*grid))\n",
    "                occ[str_og][int(node_conf[0]*10*grid)][int(node_conf[1]*10*grid) + 1] = 1\n",
    "                ini[str_og][int(s[0]*10*grid)][int(s[1]*10*grid)] = 1\n",
    "                end[str_og][int(g[0]*10*grid)][int(g[1]*10*grid)] = 1\n",
    "            i += 1 \n",
    "    for element in occ:\n",
    "        tmp_occ = state_to_numpy(element)\n",
    "        tmp_occ = tmp_occ.astype(np.uint8)\n",
    "        tmp_occ = tmp_occ.reshape(10, 10)\n",
    "        tmp_occ = tmp_occ.repeat(grid, axis = 0)\n",
    "        tmp_occ = tmp_occ.repeat(grid, axis = 1)\n",
    "        \n",
    "        data = []\n",
    "        data.append(tmp_occ)\n",
    "        data.append(occ[element])\n",
    "        if with_start_goal == True:\n",
    "            data.append(ini[element])\n",
    "            data.append(end[element])\n",
    "        data = np.array(data)\n",
    "        all_data.append(data)\n",
    "    all_data = np.array(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.vgg import VGG\n",
    "\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)g\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCN16s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class FCNs(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n",
    "        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "\n",
    "\n",
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.9, b2=0.999, batch_size=16, eps=1e-08, grid=16, lr=0.0001, max_epochs=1000, n_cpu=8)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--eps\", type=float, default=1e-8, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--grid\", type=int, default=16, help=\"the ratio to expand the environment\")\n",
    "#parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "#parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "#parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "#parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([863, 3, 160, 160])\n",
      "torch.Size([863, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DetectionSet(Dataset):\n",
    "    def __init__(self, data, with_start_goal=True, gaussian=False):\n",
    "        if with_start_goal == True:\n",
    "            \n",
    "            self.data = torch.FloatTensor(data[:,[0,2,3]])\n",
    "            self.label = torch.IntTensor(data[:,1])\n",
    "            print(self.data.shape)\n",
    "            print(self.label.shape)\n",
    "        else:\n",
    "            self.data = torch.FloatTensor(data[:,0])\n",
    "            self.label = torch.FloatTeensor(data[:, 1])\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "G = nx.read_graphml(\"graphs/dense_graph.graphml\")\n",
    "data_dir = \"dataset\"\n",
    "data = detection_data(G, data_dir, 16, with_start_goal = True)\n",
    "ratioTestTrain = 0.8\n",
    "numtotal = data.shape[0]\n",
    "train_data = data[0:int(numtotal*ratioTestTrain)]\n",
    "test_data = data[int(numtotal*ratioTestTrain):numtotal]\n",
    "train_set = DetectionSet(data = train_data, with_start_goal = True)\n",
    "train_loader = DataLoader(train_set, batch_size = opt.batch_size, shuffle = True)\n",
    "#test_set = DetectionSet(data = test_data, with_start_goal = True)\n",
    "#test_loader = DataLoader(test_set, batch_size = opt.batch_size, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG = VGGNet(pretrained = False)\n",
    "# FCN32s = FCN32s(pretrained_net = VGG, n_class = 2)\n",
    "# FCN16s = FCN16s(pretrained_net = VGG, n_class = 2)\n",
    "# FCN8s = FCN8s(pretrained_net = VGG, n_class = 2)\n",
    "# FCNs = FCNs(pretrained_net = VGG, n_class = 2)\n",
    "\n",
    "\n",
    "VGG = VGGNet(pretrained = False)\n",
    "net = FCN32s(pretrained_net = VGG, n_class = 1)\n",
    "\n",
    "resume = None\n",
    "# resume = \"\"\n",
    "if resume == None:\n",
    "    i = 9\n",
    "else:\n",
    "    net.load_state_dict(torch.load(resume))\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "net.train()\n",
    "optimizer = optim.Adam(net.parameters(), lr=opt.lr, \n",
    "                      betas=(opt.b1, opt.b2), eps=opt.eps, weight_decay=0)\n",
    "schedular = optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                      milestones = [50,200, 500, 1000, 2000], gamma = 0.1)\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter(\"/home/zhizuo/lego/tensorboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the 0 epoch\n",
      "Average loss of epoch 1 : Loss 0.19678754\n",
      "starting the 1 epoch\n",
      "Average loss of epoch 2 : Loss 0.00602691\n",
      "starting the 2 epoch\n",
      "Average loss of epoch 3 : Loss 0.00414538\n",
      "starting the 3 epoch\n",
      "Average loss of epoch 4 : Loss 0.00375230\n",
      "starting the 4 epoch\n",
      "Average loss of epoch 5 : Loss 0.00359676\n",
      "starting the 5 epoch\n",
      "Average loss of epoch 6 : Loss 0.00351847\n",
      "starting the 6 epoch\n",
      "Average loss of epoch 7 : Loss 0.00347338\n",
      "starting the 7 epoch\n",
      "Average loss of epoch 8 : Loss 0.00344576\n",
      "starting the 8 epoch\n",
      "Average loss of epoch 9 : Loss 0.00342787\n",
      "starting the 9 epoch\n",
      "Average loss of epoch 10 : Loss 0.00341207\n",
      "starting the 10 epoch\n",
      "Average loss of epoch 11 : Loss 0.00340492\n",
      "starting the 11 epoch\n",
      "Average loss of epoch 12 : Loss 0.00339600\n",
      "starting the 12 epoch\n",
      "Average loss of epoch 13 : Loss 0.00339492\n",
      "starting the 13 epoch\n",
      "Average loss of epoch 14 : Loss 0.00339080\n",
      "starting the 14 epoch\n",
      "Average loss of epoch 15 : Loss 0.00338678\n",
      "starting the 15 epoch\n",
      "Average loss of epoch 16 : Loss 0.00338561\n",
      "starting the 16 epoch\n",
      "Average loss of epoch 17 : Loss 0.00338637\n",
      "starting the 17 epoch\n",
      "Average loss of epoch 18 : Loss 0.00338104\n",
      "starting the 18 epoch\n",
      "Average loss of epoch 19 : Loss 0.00338184\n",
      "starting the 19 epoch\n",
      "Average loss of epoch 20 : Loss 0.00338034\n",
      "starting the 20 epoch\n",
      "Average loss of epoch 21 : Loss 0.00338340\n",
      "starting the 21 epoch\n",
      "Average loss of epoch 22 : Loss 0.00338443\n",
      "starting the 22 epoch\n",
      "Average loss of epoch 23 : Loss 0.00338533\n",
      "starting the 23 epoch\n",
      "Average loss of epoch 24 : Loss 0.00338629\n",
      "starting the 24 epoch\n",
      "Average loss of epoch 25 : Loss 0.00338153\n",
      "starting the 25 epoch\n",
      "Average loss of epoch 26 : Loss 0.00338300\n",
      "starting the 26 epoch\n",
      "Average loss of epoch 27 : Loss 0.00338290\n",
      "starting the 27 epoch\n",
      "Average loss of epoch 28 : Loss 0.00338718\n",
      "starting the 28 epoch\n",
      "Average loss of epoch 29 : Loss 0.00338524\n",
      "starting the 29 epoch\n",
      "Average loss of epoch 30 : Loss 0.00338922\n",
      "starting the 30 epoch\n",
      "Average loss of epoch 31 : Loss 0.00338610\n",
      "starting the 31 epoch\n",
      "Average loss of epoch 32 : Loss 0.00339192\n",
      "starting the 32 epoch\n",
      "Average loss of epoch 33 : Loss 0.00338978\n",
      "starting the 33 epoch\n",
      "Average loss of epoch 34 : Loss 0.00339195\n",
      "starting the 34 epoch\n",
      "Average loss of epoch 35 : Loss 0.00339176\n",
      "starting the 35 epoch\n",
      "Average loss of epoch 36 : Loss 0.00339030\n",
      "starting the 36 epoch\n",
      "Average loss of epoch 37 : Loss 0.00339186\n",
      "starting the 37 epoch\n",
      "Average loss of epoch 38 : Loss 0.00339552\n",
      "starting the 38 epoch\n",
      "Average loss of epoch 39 : Loss 0.00339348\n",
      "starting the 39 epoch\n",
      "Average loss of epoch 40 : Loss 0.00339614\n",
      "starting the 40 epoch\n",
      "Average loss of epoch 41 : Loss 0.00339714\n",
      "starting the 41 epoch\n",
      "Average loss of epoch 42 : Loss 0.00339129\n",
      "starting the 42 epoch\n",
      "Average loss of epoch 43 : Loss 0.00339731\n",
      "starting the 43 epoch\n",
      "Average loss of epoch 44 : Loss 0.00339592\n",
      "starting the 44 epoch\n",
      "Average loss of epoch 45 : Loss 0.00339849\n",
      "starting the 45 epoch\n",
      "Average loss of epoch 46 : Loss 0.00339034\n",
      "starting the 46 epoch\n",
      "Average loss of epoch 47 : Loss 0.00339093\n",
      "starting the 47 epoch\n",
      "Average loss of epoch 48 : Loss 0.00338945\n",
      "starting the 48 epoch\n",
      "Average loss of epoch 49 : Loss 0.00339635\n",
      "starting the 49 epoch\n",
      "Average loss of epoch 50 : Loss 0.00339180\n",
      "starting the 50 epoch\n",
      "Average loss of epoch 51 : Loss 0.00328878\n",
      "starting the 51 epoch\n",
      "Average loss of epoch 52 : Loss 0.00327652\n",
      "starting the 52 epoch\n",
      "Average loss of epoch 53 : Loss 0.00327580\n",
      "starting the 53 epoch\n",
      "Average loss of epoch 54 : Loss 0.00327438\n",
      "starting the 54 epoch\n",
      "Average loss of epoch 55 : Loss 0.00327387\n",
      "starting the 55 epoch\n",
      "Average loss of epoch 56 : Loss 0.00327384\n",
      "starting the 56 epoch\n",
      "Average loss of epoch 57 : Loss 0.00327379\n",
      "starting the 57 epoch\n",
      "Average loss of epoch 58 : Loss 0.00327304\n",
      "starting the 58 epoch\n",
      "Average loss of epoch 59 : Loss 0.00327212\n",
      "starting the 59 epoch\n",
      "Average loss of epoch 60 : Loss 0.00327294\n",
      "starting the 60 epoch\n",
      "Average loss of epoch 61 : Loss 0.00327134\n",
      "starting the 61 epoch\n",
      "Average loss of epoch 62 : Loss 0.00327117\n",
      "starting the 62 epoch\n",
      "Average loss of epoch 63 : Loss 0.00327239\n",
      "starting the 63 epoch\n",
      "Average loss of epoch 64 : Loss 0.00327069\n",
      "starting the 64 epoch\n",
      "Average loss of epoch 65 : Loss 0.00327044\n",
      "starting the 65 epoch\n",
      "Average loss of epoch 66 : Loss 0.00327010\n",
      "starting the 66 epoch\n",
      "Average loss of epoch 67 : Loss 0.00326861\n",
      "starting the 67 epoch\n",
      "Average loss of epoch 68 : Loss 0.00326933\n",
      "starting the 68 epoch\n",
      "Average loss of epoch 69 : Loss 0.00326874\n",
      "starting the 69 epoch\n",
      "Average loss of epoch 70 : Loss 0.00326678\n",
      "starting the 70 epoch\n",
      "Average loss of epoch 71 : Loss 0.00326705\n",
      "starting the 71 epoch\n",
      "Average loss of epoch 72 : Loss 0.00326773\n",
      "starting the 72 epoch\n",
      "Average loss of epoch 73 : Loss 0.00326587\n",
      "starting the 73 epoch\n",
      "Average loss of epoch 74 : Loss 0.00326518\n",
      "starting the 74 epoch\n",
      "Average loss of epoch 75 : Loss 0.00326364\n",
      "starting the 75 epoch\n",
      "Average loss of epoch 76 : Loss 0.00326459\n",
      "starting the 76 epoch\n",
      "Average loss of epoch 77 : Loss 0.00326411\n",
      "starting the 77 epoch\n",
      "Average loss of epoch 78 : Loss 0.00326418\n",
      "starting the 78 epoch\n",
      "Average loss of epoch 79 : Loss 0.00326286\n",
      "starting the 79 epoch\n",
      "Average loss of epoch 80 : Loss 0.00326231\n",
      "starting the 80 epoch\n",
      "Average loss of epoch 81 : Loss 0.00326088\n",
      "starting the 81 epoch\n",
      "Average loss of epoch 82 : Loss 0.00326195\n",
      "starting the 82 epoch\n",
      "Average loss of epoch 83 : Loss 0.00326031\n",
      "starting the 83 epoch\n",
      "Average loss of epoch 84 : Loss 0.00326055\n",
      "starting the 84 epoch\n",
      "Average loss of epoch 85 : Loss 0.00325822\n",
      "starting the 85 epoch\n",
      "Average loss of epoch 86 : Loss 0.00325872\n",
      "starting the 86 epoch\n",
      "Average loss of epoch 87 : Loss 0.00325861\n",
      "starting the 87 epoch\n",
      "Average loss of epoch 88 : Loss 0.00325750\n",
      "starting the 88 epoch\n",
      "Average loss of epoch 89 : Loss 0.00325670\n",
      "starting the 89 epoch\n",
      "Average loss of epoch 90 : Loss 0.00325675\n",
      "starting the 90 epoch\n",
      "Average loss of epoch 91 : Loss 0.00325593\n",
      "starting the 91 epoch\n",
      "Average loss of epoch 92 : Loss 0.00325518\n",
      "starting the 92 epoch\n",
      "Average loss of epoch 93 : Loss 0.00325111\n",
      "starting the 93 epoch\n",
      "Average loss of epoch 94 : Loss 0.00325299\n",
      "starting the 94 epoch\n",
      "Average loss of epoch 95 : Loss 0.00325196\n",
      "starting the 95 epoch\n",
      "Average loss of epoch 96 : Loss 0.00325033\n",
      "starting the 96 epoch\n",
      "Average loss of epoch 97 : Loss 0.00324927\n",
      "starting the 97 epoch\n",
      "Average loss of epoch 98 : Loss 0.00324944\n",
      "starting the 98 epoch\n",
      "Average loss of epoch 99 : Loss 0.00324752\n",
      "starting the 99 epoch\n",
      "Average loss of epoch 100 : Loss 0.00324676\n",
      "starting the 100 epoch\n",
      "Average loss of epoch 101 : Loss 0.00324597\n",
      "starting the 101 epoch\n",
      "Average loss of epoch 102 : Loss 0.00324533\n",
      "starting the 102 epoch\n",
      "Average loss of epoch 103 : Loss 0.00324380\n",
      "starting the 103 epoch\n",
      "Average loss of epoch 104 : Loss 0.00324366\n",
      "starting the 104 epoch\n",
      "Average loss of epoch 105 : Loss 0.00324215\n",
      "starting the 105 epoch\n",
      "Average loss of epoch 106 : Loss 0.00323951\n",
      "starting the 106 epoch\n",
      "Average loss of epoch 107 : Loss 0.00323942\n",
      "starting the 107 epoch\n",
      "Average loss of epoch 108 : Loss 0.00323846\n",
      "starting the 108 epoch\n",
      "Average loss of epoch 109 : Loss 0.00323825\n",
      "starting the 109 epoch\n",
      "Average loss of epoch 110 : Loss 0.00323473\n",
      "starting the 110 epoch\n",
      "Average loss of epoch 111 : Loss 0.00323562\n",
      "starting the 111 epoch\n",
      "Average loss of epoch 112 : Loss 0.00323263\n",
      "starting the 112 epoch\n",
      "Average loss of epoch 113 : Loss 0.00323255\n",
      "starting the 113 epoch\n",
      "Average loss of epoch 114 : Loss 0.00322790\n",
      "starting the 114 epoch\n",
      "Average loss of epoch 115 : Loss 0.00322780\n",
      "starting the 115 epoch\n",
      "Average loss of epoch 116 : Loss 0.00322652\n",
      "starting the 116 epoch\n",
      "Average loss of epoch 117 : Loss 0.00322350\n",
      "starting the 117 epoch\n",
      "Average loss of epoch 118 : Loss 0.00322357\n",
      "starting the 118 epoch\n",
      "Average loss of epoch 119 : Loss 0.00322039\n",
      "starting the 119 epoch\n",
      "Average loss of epoch 120 : Loss 0.00322163\n",
      "starting the 120 epoch\n",
      "Average loss of epoch 121 : Loss 0.00321755\n",
      "starting the 121 epoch\n",
      "Average loss of epoch 122 : Loss 0.00321867\n",
      "starting the 122 epoch\n",
      "Average loss of epoch 123 : Loss 0.00321439\n",
      "starting the 123 epoch\n",
      "Average loss of epoch 124 : Loss 0.00321371\n",
      "starting the 124 epoch\n",
      "Average loss of epoch 125 : Loss 0.00321020\n",
      "starting the 125 epoch\n",
      "Average loss of epoch 126 : Loss 0.00320905\n",
      "starting the 126 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 127 : Loss 0.00320771\n",
      "starting the 127 epoch\n",
      "Average loss of epoch 128 : Loss 0.00320452\n",
      "starting the 128 epoch\n",
      "Average loss of epoch 129 : Loss 0.00320431\n",
      "starting the 129 epoch\n",
      "Average loss of epoch 130 : Loss 0.00320466\n",
      "starting the 130 epoch\n",
      "Average loss of epoch 131 : Loss 0.00320033\n",
      "starting the 131 epoch\n",
      "Average loss of epoch 132 : Loss 0.00319831\n",
      "starting the 132 epoch\n",
      "Average loss of epoch 133 : Loss 0.00319670\n",
      "starting the 133 epoch\n",
      "Average loss of epoch 134 : Loss 0.00319547\n",
      "starting the 134 epoch\n",
      "Average loss of epoch 135 : Loss 0.00319079\n",
      "starting the 135 epoch\n",
      "Average loss of epoch 136 : Loss 0.00318891\n",
      "starting the 136 epoch\n",
      "Average loss of epoch 137 : Loss 0.00318704\n",
      "starting the 137 epoch\n",
      "Average loss of epoch 138 : Loss 0.00318324\n",
      "starting the 138 epoch\n",
      "Average loss of epoch 139 : Loss 0.00318283\n",
      "starting the 139 epoch\n",
      "Average loss of epoch 140 : Loss 0.00318192\n",
      "starting the 140 epoch\n",
      "Average loss of epoch 141 : Loss 0.00317943\n",
      "starting the 141 epoch\n",
      "Average loss of epoch 142 : Loss 0.00317680\n",
      "starting the 142 epoch\n",
      "Average loss of epoch 143 : Loss 0.00317223\n",
      "starting the 143 epoch\n",
      "Average loss of epoch 144 : Loss 0.00317029\n",
      "starting the 144 epoch\n",
      "Average loss of epoch 145 : Loss 0.00316920\n",
      "starting the 145 epoch\n",
      "Average loss of epoch 146 : Loss 0.00316494\n",
      "starting the 146 epoch\n",
      "Average loss of epoch 147 : Loss 0.00316522\n",
      "starting the 147 epoch\n",
      "Average loss of epoch 148 : Loss 0.00316013\n",
      "starting the 148 epoch\n",
      "Average loss of epoch 149 : Loss 0.00315990\n",
      "starting the 149 epoch\n",
      "Average loss of epoch 150 : Loss 0.00315581\n",
      "starting the 150 epoch\n",
      "Average loss of epoch 151 : Loss 0.00315123\n",
      "starting the 151 epoch\n",
      "Average loss of epoch 152 : Loss 0.00315171\n",
      "starting the 152 epoch\n",
      "Average loss of epoch 153 : Loss 0.00314597\n",
      "starting the 153 epoch\n",
      "Average loss of epoch 154 : Loss 0.00314154\n",
      "starting the 154 epoch\n",
      "Average loss of epoch 155 : Loss 0.00313903\n",
      "starting the 155 epoch\n",
      "Average loss of epoch 156 : Loss 0.00313663\n",
      "starting the 156 epoch\n",
      "Average loss of epoch 157 : Loss 0.00313574\n",
      "starting the 157 epoch\n",
      "Average loss of epoch 158 : Loss 0.00313202\n",
      "starting the 158 epoch\n",
      "Average loss of epoch 159 : Loss 0.00312938\n",
      "starting the 159 epoch\n",
      "Average loss of epoch 160 : Loss 0.00312602\n",
      "starting the 160 epoch\n",
      "Average loss of epoch 161 : Loss 0.00312018\n",
      "starting the 161 epoch\n",
      "Average loss of epoch 162 : Loss 0.00311993\n",
      "starting the 162 epoch\n",
      "Average loss of epoch 163 : Loss 0.00311699\n",
      "starting the 163 epoch\n",
      "Average loss of epoch 164 : Loss 0.00311362\n",
      "starting the 164 epoch\n",
      "Average loss of epoch 165 : Loss 0.00311146\n",
      "starting the 165 epoch\n",
      "Average loss of epoch 166 : Loss 0.00310607\n",
      "starting the 166 epoch\n",
      "Average loss of epoch 167 : Loss 0.00310194\n",
      "starting the 167 epoch\n",
      "Average loss of epoch 168 : Loss 0.00310009\n",
      "starting the 168 epoch\n",
      "Average loss of epoch 169 : Loss 0.00309754\n",
      "starting the 169 epoch\n",
      "Average loss of epoch 170 : Loss 0.00309032\n",
      "starting the 170 epoch\n",
      "Average loss of epoch 171 : Loss 0.00308991\n",
      "starting the 171 epoch\n",
      "Average loss of epoch 172 : Loss 0.00308552\n",
      "starting the 172 epoch\n",
      "Average loss of epoch 173 : Loss 0.00308382\n",
      "starting the 173 epoch\n",
      "Average loss of epoch 174 : Loss 0.00307649\n",
      "starting the 174 epoch\n",
      "Average loss of epoch 175 : Loss 0.00307434\n",
      "starting the 175 epoch\n",
      "Average loss of epoch 176 : Loss 0.00307129\n",
      "starting the 176 epoch\n",
      "Average loss of epoch 177 : Loss 0.00306623\n",
      "starting the 177 epoch\n",
      "Average loss of epoch 178 : Loss 0.00306629\n",
      "starting the 178 epoch\n",
      "Average loss of epoch 179 : Loss 0.00306320\n",
      "starting the 179 epoch\n",
      "Average loss of epoch 180 : Loss 0.00306270\n",
      "starting the 180 epoch\n",
      "Average loss of epoch 181 : Loss 0.00305397\n",
      "starting the 181 epoch\n",
      "Average loss of epoch 182 : Loss 0.00305033\n",
      "starting the 182 epoch\n",
      "Average loss of epoch 183 : Loss 0.00304641\n",
      "starting the 183 epoch\n",
      "Average loss of epoch 184 : Loss 0.00304347\n",
      "starting the 184 epoch\n",
      "Average loss of epoch 185 : Loss 0.00303879\n",
      "starting the 185 epoch\n",
      "Average loss of epoch 186 : Loss 0.00303519\n",
      "starting the 186 epoch\n",
      "Average loss of epoch 187 : Loss 0.00302855\n",
      "starting the 187 epoch\n",
      "Average loss of epoch 188 : Loss 0.00303052\n",
      "starting the 188 epoch\n",
      "Average loss of epoch 189 : Loss 0.00302344\n",
      "starting the 189 epoch\n",
      "Average loss of epoch 190 : Loss 0.00301938\n",
      "starting the 190 epoch\n",
      "Average loss of epoch 191 : Loss 0.00301777\n",
      "starting the 191 epoch\n",
      "Average loss of epoch 192 : Loss 0.00301344\n",
      "starting the 192 epoch\n",
      "Average loss of epoch 193 : Loss 0.00301151\n",
      "starting the 193 epoch\n",
      "Average loss of epoch 194 : Loss 0.00300194\n",
      "starting the 194 epoch\n",
      "Average loss of epoch 195 : Loss 0.00299877\n",
      "starting the 195 epoch\n",
      "Average loss of epoch 196 : Loss 0.00299610\n",
      "starting the 196 epoch\n",
      "Average loss of epoch 197 : Loss 0.00299517\n",
      "starting the 197 epoch\n",
      "Average loss of epoch 198 : Loss 0.00299115\n",
      "starting the 198 epoch\n",
      "Average loss of epoch 199 : Loss 0.00298897\n",
      "starting the 199 epoch\n",
      "Average loss of epoch 200 : Loss 0.00297961\n",
      "starting the 200 epoch\n",
      "Average loss of epoch 201 : Loss 0.00294646\n",
      "starting the 201 epoch\n",
      "Average loss of epoch 202 : Loss 0.00293952\n",
      "starting the 202 epoch\n",
      "Average loss of epoch 203 : Loss 0.00293926\n",
      "starting the 203 epoch\n",
      "Average loss of epoch 204 : Loss 0.00293872\n",
      "starting the 204 epoch\n",
      "Average loss of epoch 205 : Loss 0.00293842\n",
      "starting the 205 epoch\n",
      "Average loss of epoch 206 : Loss 0.00293894\n",
      "starting the 206 epoch\n",
      "Average loss of epoch 207 : Loss 0.00293832\n",
      "starting the 207 epoch\n",
      "Average loss of epoch 208 : Loss 0.00293580\n",
      "starting the 208 epoch\n",
      "Average loss of epoch 209 : Loss 0.00293648\n",
      "starting the 209 epoch\n",
      "Average loss of epoch 210 : Loss 0.00293499\n",
      "starting the 210 epoch\n",
      "Average loss of epoch 211 : Loss 0.00293567\n",
      "starting the 211 epoch\n",
      "Average loss of epoch 212 : Loss 0.00293409\n",
      "starting the 212 epoch\n",
      "Average loss of epoch 213 : Loss 0.00293457\n",
      "starting the 213 epoch\n",
      "Average loss of epoch 214 : Loss 0.00293324\n",
      "starting the 214 epoch\n",
      "Average loss of epoch 215 : Loss 0.00293359\n",
      "starting the 215 epoch\n",
      "Average loss of epoch 216 : Loss 0.00293354\n",
      "starting the 216 epoch\n",
      "Average loss of epoch 217 : Loss 0.00293139\n",
      "starting the 217 epoch\n",
      "Average loss of epoch 218 : Loss 0.00293269\n",
      "starting the 218 epoch\n",
      "Average loss of epoch 219 : Loss 0.00293135\n",
      "starting the 219 epoch\n",
      "Average loss of epoch 220 : Loss 0.00293150\n",
      "starting the 220 epoch\n",
      "Average loss of epoch 221 : Loss 0.00293107\n",
      "starting the 221 epoch\n",
      "Average loss of epoch 222 : Loss 0.00292974\n",
      "starting the 222 epoch\n",
      "Average loss of epoch 223 : Loss 0.00292948\n",
      "starting the 223 epoch\n",
      "Average loss of epoch 224 : Loss 0.00292921\n",
      "starting the 224 epoch\n",
      "Average loss of epoch 225 : Loss 0.00292854\n",
      "starting the 225 epoch\n",
      "Average loss of epoch 226 : Loss 0.00292791\n",
      "starting the 226 epoch\n",
      "Average loss of epoch 227 : Loss 0.00292805\n",
      "starting the 227 epoch\n",
      "Average loss of epoch 228 : Loss 0.00292677\n",
      "starting the 228 epoch\n",
      "Average loss of epoch 229 : Loss 0.00292561\n",
      "starting the 229 epoch\n",
      "Average loss of epoch 230 : Loss 0.00292658\n",
      "starting the 230 epoch\n",
      "Average loss of epoch 231 : Loss 0.00292534\n",
      "starting the 231 epoch\n",
      "Average loss of epoch 232 : Loss 0.00292385\n",
      "starting the 232 epoch\n",
      "Average loss of epoch 233 : Loss 0.00292359\n",
      "starting the 233 epoch\n",
      "Average loss of epoch 234 : Loss 0.00292387\n",
      "starting the 234 epoch\n",
      "Average loss of epoch 235 : Loss 0.00292240\n",
      "starting the 235 epoch\n",
      "Average loss of epoch 236 : Loss 0.00292363\n",
      "starting the 236 epoch\n",
      "Average loss of epoch 237 : Loss 0.00292159\n",
      "starting the 237 epoch\n",
      "Average loss of epoch 238 : Loss 0.00292059\n",
      "starting the 238 epoch\n",
      "Average loss of epoch 239 : Loss 0.00292006\n",
      "starting the 239 epoch\n",
      "Average loss of epoch 240 : Loss 0.00291974\n",
      "starting the 240 epoch\n",
      "Average loss of epoch 241 : Loss 0.00291941\n",
      "starting the 241 epoch\n",
      "Average loss of epoch 242 : Loss 0.00291849\n",
      "starting the 242 epoch\n",
      "Average loss of epoch 243 : Loss 0.00291875\n",
      "starting the 243 epoch\n",
      "Average loss of epoch 244 : Loss 0.00291679\n",
      "starting the 244 epoch\n",
      "Average loss of epoch 245 : Loss 0.00291761\n",
      "starting the 245 epoch\n",
      "Average loss of epoch 246 : Loss 0.00291573\n",
      "starting the 246 epoch\n",
      "Average loss of epoch 247 : Loss 0.00291541\n",
      "starting the 247 epoch\n",
      "Average loss of epoch 248 : Loss 0.00291489\n",
      "starting the 248 epoch\n",
      "Average loss of epoch 249 : Loss 0.00291373\n",
      "starting the 249 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 250 : Loss 0.00291387\n",
      "starting the 250 epoch\n",
      "Average loss of epoch 251 : Loss 0.00291297\n",
      "starting the 251 epoch\n",
      "Average loss of epoch 252 : Loss 0.00291179\n",
      "starting the 252 epoch\n",
      "Average loss of epoch 253 : Loss 0.00291117\n",
      "starting the 253 epoch\n",
      "Average loss of epoch 254 : Loss 0.00291051\n",
      "starting the 254 epoch\n",
      "Average loss of epoch 255 : Loss 0.00290974\n",
      "starting the 255 epoch\n",
      "Average loss of epoch 256 : Loss 0.00290969\n",
      "starting the 256 epoch\n",
      "Average loss of epoch 257 : Loss 0.00290962\n",
      "starting the 257 epoch\n",
      "Average loss of epoch 258 : Loss 0.00290786\n",
      "starting the 258 epoch\n",
      "Average loss of epoch 259 : Loss 0.00290737\n",
      "starting the 259 epoch\n",
      "Average loss of epoch 260 : Loss 0.00290675\n",
      "starting the 260 epoch\n",
      "Average loss of epoch 261 : Loss 0.00290776\n",
      "starting the 261 epoch\n",
      "Average loss of epoch 262 : Loss 0.00290554\n",
      "starting the 262 epoch\n",
      "Average loss of epoch 263 : Loss 0.00290451\n",
      "starting the 263 epoch\n",
      "Average loss of epoch 264 : Loss 0.00290386\n",
      "starting the 264 epoch\n",
      "Average loss of epoch 265 : Loss 0.00290365\n",
      "starting the 265 epoch\n",
      "Average loss of epoch 266 : Loss 0.00290278\n",
      "starting the 266 epoch\n",
      "Average loss of epoch 267 : Loss 0.00290194\n",
      "starting the 267 epoch\n",
      "Average loss of epoch 268 : Loss 0.00290209\n",
      "starting the 268 epoch\n",
      "Average loss of epoch 269 : Loss 0.00290023\n",
      "starting the 269 epoch\n",
      "Average loss of epoch 270 : Loss 0.00289997\n",
      "starting the 270 epoch\n",
      "Average loss of epoch 271 : Loss 0.00289812\n",
      "starting the 271 epoch\n",
      "Average loss of epoch 272 : Loss 0.00289813\n",
      "starting the 272 epoch\n",
      "Average loss of epoch 273 : Loss 0.00289879\n",
      "starting the 273 epoch\n",
      "Average loss of epoch 274 : Loss 0.00289821\n",
      "starting the 274 epoch\n",
      "Average loss of epoch 275 : Loss 0.00289721\n",
      "starting the 275 epoch\n",
      "Average loss of epoch 276 : Loss 0.00289712\n",
      "starting the 276 epoch\n",
      "Average loss of epoch 277 : Loss 0.00289640\n",
      "starting the 277 epoch\n",
      "Average loss of epoch 278 : Loss 0.00289483\n",
      "starting the 278 epoch\n",
      "Average loss of epoch 279 : Loss 0.00289393\n",
      "starting the 279 epoch\n",
      "Average loss of epoch 280 : Loss 0.00289315\n",
      "starting the 280 epoch\n",
      "Average loss of epoch 281 : Loss 0.00289305\n",
      "starting the 281 epoch\n",
      "Average loss of epoch 282 : Loss 0.00289207\n",
      "starting the 282 epoch\n",
      "Average loss of epoch 283 : Loss 0.00289198\n",
      "starting the 283 epoch\n",
      "Average loss of epoch 284 : Loss 0.00289192\n",
      "starting the 284 epoch\n",
      "Average loss of epoch 285 : Loss 0.00288989\n",
      "starting the 285 epoch\n",
      "Average loss of epoch 286 : Loss 0.00288924\n",
      "starting the 286 epoch\n",
      "Average loss of epoch 287 : Loss 0.00288888\n",
      "starting the 287 epoch\n",
      "Average loss of epoch 288 : Loss 0.00288759\n",
      "starting the 288 epoch\n",
      "Average loss of epoch 289 : Loss 0.00288697\n",
      "starting the 289 epoch\n",
      "Average loss of epoch 290 : Loss 0.00288703\n",
      "starting the 290 epoch\n",
      "Average loss of epoch 291 : Loss 0.00288677\n",
      "starting the 291 epoch\n",
      "Average loss of epoch 292 : Loss 0.00288484\n",
      "starting the 292 epoch\n",
      "Average loss of epoch 293 : Loss 0.00288452\n",
      "starting the 293 epoch\n",
      "Average loss of epoch 294 : Loss 0.00288369\n",
      "starting the 294 epoch\n",
      "Average loss of epoch 295 : Loss 0.00288473\n",
      "starting the 295 epoch\n",
      "Average loss of epoch 296 : Loss 0.00288301\n",
      "starting the 296 epoch\n",
      "Average loss of epoch 297 : Loss 0.00288171\n",
      "starting the 297 epoch\n",
      "Average loss of epoch 298 : Loss 0.00288169\n",
      "starting the 298 epoch\n",
      "Average loss of epoch 299 : Loss 0.00288120\n",
      "starting the 299 epoch\n",
      "Average loss of epoch 300 : Loss 0.00287915\n",
      "starting the 300 epoch\n",
      "Average loss of epoch 301 : Loss 0.00287874\n",
      "starting the 301 epoch\n",
      "Average loss of epoch 302 : Loss 0.00287861\n",
      "starting the 302 epoch\n",
      "Average loss of epoch 303 : Loss 0.00287762\n",
      "starting the 303 epoch\n",
      "Average loss of epoch 304 : Loss 0.00287802\n",
      "starting the 304 epoch\n",
      "Average loss of epoch 305 : Loss 0.00287647\n",
      "starting the 305 epoch\n",
      "Average loss of epoch 306 : Loss 0.00287545\n",
      "starting the 306 epoch\n",
      "Average loss of epoch 307 : Loss 0.00287582\n",
      "starting the 307 epoch\n",
      "Average loss of epoch 308 : Loss 0.00287429\n",
      "starting the 308 epoch\n",
      "Average loss of epoch 309 : Loss 0.00287470\n",
      "starting the 309 epoch\n",
      "Average loss of epoch 310 : Loss 0.00287369\n",
      "starting the 310 epoch\n",
      "Average loss of epoch 311 : Loss 0.00287256\n",
      "starting the 311 epoch\n",
      "Average loss of epoch 312 : Loss 0.00287186\n",
      "starting the 312 epoch\n",
      "Average loss of epoch 313 : Loss 0.00287102\n",
      "starting the 313 epoch\n",
      "Average loss of epoch 314 : Loss 0.00287065\n",
      "starting the 314 epoch\n",
      "Average loss of epoch 315 : Loss 0.00286931\n",
      "starting the 315 epoch\n",
      "Average loss of epoch 316 : Loss 0.00286909\n",
      "starting the 316 epoch\n",
      "Average loss of epoch 317 : Loss 0.00286832\n",
      "starting the 317 epoch\n",
      "Average loss of epoch 318 : Loss 0.00286824\n",
      "starting the 318 epoch\n",
      "Average loss of epoch 319 : Loss 0.00286728\n",
      "starting the 319 epoch\n",
      "Average loss of epoch 320 : Loss 0.00286599\n",
      "starting the 320 epoch\n",
      "Average loss of epoch 321 : Loss 0.00286503\n",
      "starting the 321 epoch\n",
      "Average loss of epoch 322 : Loss 0.00286510\n",
      "starting the 322 epoch\n",
      "Average loss of epoch 323 : Loss 0.00286532\n",
      "starting the 323 epoch\n",
      "Average loss of epoch 324 : Loss 0.00286377\n",
      "starting the 324 epoch\n",
      "Average loss of epoch 325 : Loss 0.00286336\n",
      "starting the 325 epoch\n",
      "Average loss of epoch 326 : Loss 0.00286277\n",
      "starting the 326 epoch\n",
      "Average loss of epoch 327 : Loss 0.00286208\n",
      "starting the 327 epoch\n",
      "Average loss of epoch 328 : Loss 0.00286061\n",
      "starting the 328 epoch\n",
      "Average loss of epoch 329 : Loss 0.00286073\n",
      "starting the 329 epoch\n",
      "Average loss of epoch 330 : Loss 0.00286049\n",
      "starting the 330 epoch\n",
      "Average loss of epoch 331 : Loss 0.00285828\n",
      "starting the 331 epoch\n",
      "Average loss of epoch 332 : Loss 0.00285899\n",
      "starting the 332 epoch\n",
      "Average loss of epoch 333 : Loss 0.00285820\n",
      "starting the 333 epoch\n",
      "Average loss of epoch 334 : Loss 0.00285711\n",
      "starting the 334 epoch\n",
      "Average loss of epoch 335 : Loss 0.00285649\n",
      "starting the 335 epoch\n",
      "Average loss of epoch 336 : Loss 0.00285492\n",
      "starting the 336 epoch\n",
      "Average loss of epoch 337 : Loss 0.00285440\n",
      "starting the 337 epoch\n",
      "Average loss of epoch 338 : Loss 0.00285432\n",
      "starting the 338 epoch\n",
      "Average loss of epoch 339 : Loss 0.00285317\n",
      "starting the 339 epoch\n",
      "Average loss of epoch 340 : Loss 0.00285203\n",
      "starting the 340 epoch\n",
      "Average loss of epoch 341 : Loss 0.00285311\n",
      "starting the 341 epoch\n",
      "Average loss of epoch 342 : Loss 0.00285177\n",
      "starting the 342 epoch\n",
      "Average loss of epoch 343 : Loss 0.00285104\n",
      "starting the 343 epoch\n",
      "Average loss of epoch 344 : Loss 0.00284994\n",
      "starting the 344 epoch\n",
      "Average loss of epoch 345 : Loss 0.00284953\n",
      "starting the 345 epoch\n",
      "Average loss of epoch 346 : Loss 0.00284872\n",
      "starting the 346 epoch\n",
      "Average loss of epoch 347 : Loss 0.00284954\n",
      "starting the 347 epoch\n",
      "Average loss of epoch 348 : Loss 0.00284774\n",
      "starting the 348 epoch\n",
      "Average loss of epoch 349 : Loss 0.00284709\n",
      "starting the 349 epoch\n",
      "Average loss of epoch 350 : Loss 0.00284555\n",
      "starting the 350 epoch\n",
      "Average loss of epoch 351 : Loss 0.00284591\n",
      "starting the 351 epoch\n",
      "Average loss of epoch 352 : Loss 0.00284539\n",
      "starting the 352 epoch\n",
      "Average loss of epoch 353 : Loss 0.00284477\n",
      "starting the 353 epoch\n",
      "Average loss of epoch 354 : Loss 0.00284264\n",
      "starting the 354 epoch\n",
      "Average loss of epoch 355 : Loss 0.00284328\n",
      "starting the 355 epoch\n",
      "Average loss of epoch 356 : Loss 0.00284261\n",
      "starting the 356 epoch\n",
      "Average loss of epoch 357 : Loss 0.00284149\n",
      "starting the 357 epoch\n",
      "Average loss of epoch 358 : Loss 0.00284065\n",
      "starting the 358 epoch\n",
      "Average loss of epoch 359 : Loss 0.00284088\n",
      "starting the 359 epoch\n",
      "Average loss of epoch 360 : Loss 0.00284003\n",
      "starting the 360 epoch\n",
      "Average loss of epoch 361 : Loss 0.00283941\n",
      "starting the 361 epoch\n",
      "Average loss of epoch 362 : Loss 0.00283862\n",
      "starting the 362 epoch\n",
      "Average loss of epoch 363 : Loss 0.00283862\n",
      "starting the 363 epoch\n",
      "Average loss of epoch 364 : Loss 0.00283660\n",
      "starting the 364 epoch\n",
      "Average loss of epoch 365 : Loss 0.00283700\n",
      "starting the 365 epoch\n",
      "Average loss of epoch 366 : Loss 0.00283601\n",
      "starting the 366 epoch\n",
      "Average loss of epoch 367 : Loss 0.00283490\n",
      "starting the 367 epoch\n",
      "Average loss of epoch 368 : Loss 0.00283354\n",
      "starting the 368 epoch\n",
      "Average loss of epoch 369 : Loss 0.00283494\n",
      "starting the 369 epoch\n",
      "Average loss of epoch 370 : Loss 0.00283237\n",
      "starting the 370 epoch\n",
      "Average loss of epoch 371 : Loss 0.00283194\n",
      "starting the 371 epoch\n",
      "Average loss of epoch 372 : Loss 0.00283218\n",
      "starting the 372 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 373 : Loss 0.00283203\n",
      "starting the 373 epoch\n",
      "Average loss of epoch 374 : Loss 0.00283077\n",
      "starting the 374 epoch\n",
      "Average loss of epoch 375 : Loss 0.00283146\n",
      "starting the 375 epoch\n",
      "Average loss of epoch 376 : Loss 0.00282888\n",
      "starting the 376 epoch\n",
      "Average loss of epoch 377 : Loss 0.00282984\n",
      "starting the 377 epoch\n",
      "Average loss of epoch 378 : Loss 0.00282800\n",
      "starting the 378 epoch\n",
      "Average loss of epoch 379 : Loss 0.00282743\n",
      "starting the 379 epoch\n",
      "Average loss of epoch 380 : Loss 0.00282600\n",
      "starting the 380 epoch\n",
      "Average loss of epoch 381 : Loss 0.00282548\n",
      "starting the 381 epoch\n",
      "Average loss of epoch 382 : Loss 0.00282517\n",
      "starting the 382 epoch\n",
      "Average loss of epoch 383 : Loss 0.00282463\n",
      "starting the 383 epoch\n",
      "Average loss of epoch 384 : Loss 0.00282403\n",
      "starting the 384 epoch\n",
      "Average loss of epoch 385 : Loss 0.00282426\n",
      "starting the 385 epoch\n",
      "Average loss of epoch 386 : Loss 0.00282178\n",
      "starting the 386 epoch\n",
      "Average loss of epoch 387 : Loss 0.00282144\n",
      "starting the 387 epoch\n",
      "Average loss of epoch 388 : Loss 0.00282161\n",
      "starting the 388 epoch\n",
      "Average loss of epoch 389 : Loss 0.00282169\n",
      "starting the 389 epoch\n",
      "Average loss of epoch 390 : Loss 0.00282135\n",
      "starting the 390 epoch\n",
      "Average loss of epoch 391 : Loss 0.00281927\n",
      "starting the 391 epoch\n",
      "Average loss of epoch 392 : Loss 0.00281894\n",
      "starting the 392 epoch\n",
      "Average loss of epoch 393 : Loss 0.00281706\n",
      "starting the 393 epoch\n",
      "Average loss of epoch 394 : Loss 0.00281756\n",
      "starting the 394 epoch\n",
      "Average loss of epoch 395 : Loss 0.00281670\n",
      "starting the 395 epoch\n",
      "Average loss of epoch 396 : Loss 0.00281589\n",
      "starting the 396 epoch\n",
      "Average loss of epoch 397 : Loss 0.00281556\n",
      "starting the 397 epoch\n",
      "Average loss of epoch 398 : Loss 0.00281469\n",
      "starting the 398 epoch\n",
      "Average loss of epoch 399 : Loss 0.00281480\n",
      "starting the 399 epoch\n",
      "Average loss of epoch 400 : Loss 0.00281387\n",
      "starting the 400 epoch\n",
      "Average loss of epoch 401 : Loss 0.00281318\n",
      "starting the 401 epoch\n",
      "Average loss of epoch 402 : Loss 0.00281355\n",
      "starting the 402 epoch\n",
      "Average loss of epoch 403 : Loss 0.00281316\n",
      "starting the 403 epoch\n",
      "Average loss of epoch 404 : Loss 0.00281209\n",
      "starting the 404 epoch\n",
      "Average loss of epoch 405 : Loss 0.00281003\n",
      "starting the 405 epoch\n",
      "Average loss of epoch 406 : Loss 0.00280995\n",
      "starting the 406 epoch\n",
      "Average loss of epoch 407 : Loss 0.00281000\n",
      "starting the 407 epoch\n",
      "Average loss of epoch 408 : Loss 0.00280852\n",
      "starting the 408 epoch\n",
      "Average loss of epoch 409 : Loss 0.00280749\n",
      "starting the 409 epoch\n",
      "Average loss of epoch 410 : Loss 0.00280771\n",
      "starting the 410 epoch\n",
      "Average loss of epoch 411 : Loss 0.00280678\n",
      "starting the 411 epoch\n",
      "Average loss of epoch 412 : Loss 0.00280671\n",
      "starting the 412 epoch\n",
      "Average loss of epoch 413 : Loss 0.00280655\n",
      "starting the 413 epoch\n",
      "Average loss of epoch 414 : Loss 0.00280518\n",
      "starting the 414 epoch\n",
      "Average loss of epoch 415 : Loss 0.00280386\n",
      "starting the 415 epoch\n",
      "Average loss of epoch 416 : Loss 0.00280388\n",
      "starting the 416 epoch\n",
      "Average loss of epoch 417 : Loss 0.00280338\n",
      "starting the 417 epoch\n",
      "Average loss of epoch 418 : Loss 0.00280166\n",
      "starting the 418 epoch\n",
      "Average loss of epoch 419 : Loss 0.00280187\n",
      "starting the 419 epoch\n",
      "Average loss of epoch 420 : Loss 0.00280157\n",
      "starting the 420 epoch\n",
      "Average loss of epoch 421 : Loss 0.00280018\n",
      "starting the 421 epoch\n",
      "Average loss of epoch 422 : Loss 0.00280025\n",
      "starting the 422 epoch\n",
      "Average loss of epoch 423 : Loss 0.00279951\n",
      "starting the 423 epoch\n",
      "Average loss of epoch 424 : Loss 0.00279830\n",
      "starting the 424 epoch\n",
      "Average loss of epoch 425 : Loss 0.00279769\n",
      "starting the 425 epoch\n",
      "Average loss of epoch 426 : Loss 0.00279709\n",
      "starting the 426 epoch\n",
      "Average loss of epoch 427 : Loss 0.00279695\n",
      "starting the 427 epoch\n",
      "Average loss of epoch 428 : Loss 0.00279609\n",
      "starting the 428 epoch\n",
      "Average loss of epoch 429 : Loss 0.00279508\n",
      "starting the 429 epoch\n",
      "Average loss of epoch 430 : Loss 0.00279551\n",
      "starting the 430 epoch\n",
      "Average loss of epoch 431 : Loss 0.00279411\n",
      "starting the 431 epoch\n",
      "Average loss of epoch 432 : Loss 0.00279366\n",
      "starting the 432 epoch\n",
      "Average loss of epoch 433 : Loss 0.00279285\n",
      "starting the 433 epoch\n",
      "Average loss of epoch 434 : Loss 0.00279164\n",
      "starting the 434 epoch\n",
      "Average loss of epoch 435 : Loss 0.00279226\n",
      "starting the 435 epoch\n",
      "Average loss of epoch 436 : Loss 0.00279072\n",
      "starting the 436 epoch\n",
      "Average loss of epoch 437 : Loss 0.00279039\n",
      "starting the 437 epoch\n",
      "Average loss of epoch 438 : Loss 0.00278905\n",
      "starting the 438 epoch\n",
      "Average loss of epoch 439 : Loss 0.00279015\n",
      "starting the 439 epoch\n",
      "Average loss of epoch 440 : Loss 0.00278828\n",
      "starting the 440 epoch\n",
      "Average loss of epoch 441 : Loss 0.00278733\n",
      "starting the 441 epoch\n",
      "Average loss of epoch 442 : Loss 0.00278810\n",
      "starting the 442 epoch\n",
      "Average loss of epoch 443 : Loss 0.00278767\n",
      "starting the 443 epoch\n",
      "Average loss of epoch 444 : Loss 0.00278661\n",
      "starting the 444 epoch\n",
      "Average loss of epoch 445 : Loss 0.00278537\n",
      "starting the 445 epoch\n",
      "Average loss of epoch 446 : Loss 0.00278464\n",
      "starting the 446 epoch\n",
      "Average loss of epoch 447 : Loss 0.00278347\n",
      "starting the 447 epoch\n",
      "Average loss of epoch 448 : Loss 0.00278266\n",
      "starting the 448 epoch\n",
      "Average loss of epoch 449 : Loss 0.00278313\n",
      "starting the 449 epoch\n",
      "Average loss of epoch 450 : Loss 0.00278313\n",
      "starting the 450 epoch\n",
      "Average loss of epoch 451 : Loss 0.00278144\n",
      "starting the 451 epoch\n",
      "Average loss of epoch 452 : Loss 0.00278139\n",
      "starting the 452 epoch\n",
      "Average loss of epoch 453 : Loss 0.00277971\n",
      "starting the 453 epoch\n",
      "Average loss of epoch 454 : Loss 0.00277877\n",
      "starting the 454 epoch\n",
      "Average loss of epoch 455 : Loss 0.00277955\n",
      "starting the 455 epoch\n",
      "Average loss of epoch 456 : Loss 0.00277803\n",
      "starting the 456 epoch\n",
      "Average loss of epoch 457 : Loss 0.00277795\n",
      "starting the 457 epoch\n",
      "Average loss of epoch 458 : Loss 0.00277807\n",
      "starting the 458 epoch\n",
      "Average loss of epoch 459 : Loss 0.00277885\n",
      "starting the 459 epoch\n",
      "Average loss of epoch 460 : Loss 0.00277566\n",
      "starting the 460 epoch\n",
      "Average loss of epoch 461 : Loss 0.00277542\n",
      "starting the 461 epoch\n",
      "Average loss of epoch 462 : Loss 0.00277441\n",
      "starting the 462 epoch\n",
      "Average loss of epoch 463 : Loss 0.00277451\n",
      "starting the 463 epoch\n",
      "Average loss of epoch 464 : Loss 0.00277337\n",
      "starting the 464 epoch\n",
      "Average loss of epoch 465 : Loss 0.00277303\n",
      "starting the 465 epoch\n",
      "Average loss of epoch 466 : Loss 0.00277186\n",
      "starting the 466 epoch\n",
      "Average loss of epoch 467 : Loss 0.00277150\n",
      "starting the 467 epoch\n",
      "Average loss of epoch 468 : Loss 0.00277078\n",
      "starting the 468 epoch\n",
      "Average loss of epoch 469 : Loss 0.00276998\n",
      "starting the 469 epoch\n",
      "Average loss of epoch 470 : Loss 0.00277034\n",
      "starting the 470 epoch\n",
      "Average loss of epoch 471 : Loss 0.00276972\n",
      "starting the 471 epoch\n",
      "Average loss of epoch 472 : Loss 0.00276917\n",
      "starting the 472 epoch\n",
      "Average loss of epoch 473 : Loss 0.00276775\n",
      "starting the 473 epoch\n",
      "Average loss of epoch 474 : Loss 0.00276852\n",
      "starting the 474 epoch\n",
      "Average loss of epoch 475 : Loss 0.00276723\n",
      "starting the 475 epoch\n",
      "Average loss of epoch 476 : Loss 0.00276655\n",
      "starting the 476 epoch\n",
      "Average loss of epoch 477 : Loss 0.00276567\n",
      "starting the 477 epoch\n",
      "Average loss of epoch 478 : Loss 0.00276588\n",
      "starting the 478 epoch\n",
      "Average loss of epoch 479 : Loss 0.00276462\n",
      "starting the 479 epoch\n",
      "Average loss of epoch 480 : Loss 0.00276303\n",
      "starting the 480 epoch\n",
      "Average loss of epoch 481 : Loss 0.00276299\n",
      "starting the 481 epoch\n",
      "Average loss of epoch 482 : Loss 0.00276192\n",
      "starting the 482 epoch\n",
      "Average loss of epoch 483 : Loss 0.00276128\n",
      "starting the 483 epoch\n",
      "Average loss of epoch 484 : Loss 0.00276044\n",
      "starting the 484 epoch\n",
      "Average loss of epoch 485 : Loss 0.00276045\n",
      "starting the 485 epoch\n",
      "Average loss of epoch 486 : Loss 0.00275996\n",
      "starting the 486 epoch\n",
      "Average loss of epoch 487 : Loss 0.00275863\n",
      "starting the 487 epoch\n",
      "Average loss of epoch 488 : Loss 0.00275921\n",
      "starting the 488 epoch\n",
      "Average loss of epoch 489 : Loss 0.00275911\n",
      "starting the 489 epoch\n",
      "Average loss of epoch 490 : Loss 0.00275713\n",
      "starting the 490 epoch\n",
      "Average loss of epoch 491 : Loss 0.00275766\n",
      "starting the 491 epoch\n",
      "Average loss of epoch 492 : Loss 0.00275664\n",
      "starting the 492 epoch\n",
      "Average loss of epoch 493 : Loss 0.00275625\n",
      "starting the 493 epoch\n",
      "Average loss of epoch 494 : Loss 0.00275582\n",
      "starting the 494 epoch\n",
      "Average loss of epoch 495 : Loss 0.00275539\n",
      "starting the 495 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 496 : Loss 0.00275439\n",
      "starting the 496 epoch\n",
      "Average loss of epoch 497 : Loss 0.00275255\n",
      "starting the 497 epoch\n",
      "Average loss of epoch 498 : Loss 0.00275246\n",
      "starting the 498 epoch\n",
      "Average loss of epoch 499 : Loss 0.00275216\n",
      "starting the 499 epoch\n",
      "Average loss of epoch 500 : Loss 0.00275149\n",
      "starting the 500 epoch\n",
      "Average loss of epoch 501 : Loss 0.00274628\n",
      "starting the 501 epoch\n",
      "Average loss of epoch 502 : Loss 0.00274429\n",
      "starting the 502 epoch\n",
      "Average loss of epoch 503 : Loss 0.00274562\n",
      "starting the 503 epoch\n",
      "Average loss of epoch 504 : Loss 0.00274381\n",
      "starting the 504 epoch\n",
      "Average loss of epoch 505 : Loss 0.00274443\n",
      "starting the 505 epoch\n",
      "Average loss of epoch 506 : Loss 0.00274374\n",
      "starting the 506 epoch\n",
      "Average loss of epoch 507 : Loss 0.00274331\n",
      "starting the 507 epoch\n",
      "Average loss of epoch 508 : Loss 0.00274468\n",
      "starting the 508 epoch\n",
      "Average loss of epoch 509 : Loss 0.00274497\n",
      "starting the 509 epoch\n",
      "Average loss of epoch 510 : Loss 0.00274293\n",
      "starting the 510 epoch\n",
      "Average loss of epoch 511 : Loss 0.00274372\n",
      "starting the 511 epoch\n",
      "Average loss of epoch 512 : Loss 0.00274312\n",
      "starting the 512 epoch\n",
      "Average loss of epoch 513 : Loss 0.00274268\n",
      "starting the 513 epoch\n",
      "Average loss of epoch 514 : Loss 0.00274335\n",
      "starting the 514 epoch\n",
      "Average loss of epoch 515 : Loss 0.00274324\n",
      "starting the 515 epoch\n",
      "Average loss of epoch 516 : Loss 0.00274295\n",
      "starting the 516 epoch\n",
      "Average loss of epoch 517 : Loss 0.00274315\n",
      "starting the 517 epoch\n",
      "Average loss of epoch 518 : Loss 0.00274355\n",
      "starting the 518 epoch\n",
      "Average loss of epoch 519 : Loss 0.00274325\n",
      "starting the 519 epoch\n",
      "Average loss of epoch 520 : Loss 0.00274284\n",
      "starting the 520 epoch\n",
      "Average loss of epoch 521 : Loss 0.00274263\n",
      "starting the 521 epoch\n",
      "Average loss of epoch 522 : Loss 0.00274287\n",
      "starting the 522 epoch\n",
      "Average loss of epoch 523 : Loss 0.00274304\n",
      "starting the 523 epoch\n",
      "Average loss of epoch 524 : Loss 0.00274265\n",
      "starting the 524 epoch\n",
      "Average loss of epoch 525 : Loss 0.00274261\n",
      "starting the 525 epoch\n",
      "Average loss of epoch 526 : Loss 0.00274281\n",
      "starting the 526 epoch\n",
      "Average loss of epoch 527 : Loss 0.00274220\n",
      "starting the 527 epoch\n",
      "Average loss of epoch 528 : Loss 0.00274188\n",
      "starting the 528 epoch\n",
      "Average loss of epoch 529 : Loss 0.00274294\n",
      "starting the 529 epoch\n",
      "Average loss of epoch 530 : Loss 0.00274287\n",
      "starting the 530 epoch\n",
      "Average loss of epoch 531 : Loss 0.00274284\n",
      "starting the 531 epoch\n",
      "Average loss of epoch 532 : Loss 0.00274209\n",
      "starting the 532 epoch\n",
      "Average loss of epoch 533 : Loss 0.00274309\n",
      "starting the 533 epoch\n",
      "Average loss of epoch 534 : Loss 0.00274166\n",
      "starting the 534 epoch\n",
      "Average loss of epoch 535 : Loss 0.00274198\n",
      "starting the 535 epoch\n",
      "Average loss of epoch 536 : Loss 0.00274182\n",
      "starting the 536 epoch\n",
      "Average loss of epoch 537 : Loss 0.00274166\n",
      "starting the 537 epoch\n",
      "Average loss of epoch 538 : Loss 0.00274158\n",
      "starting the 538 epoch\n",
      "Average loss of epoch 539 : Loss 0.00274167\n",
      "starting the 539 epoch\n",
      "Average loss of epoch 540 : Loss 0.00274139\n",
      "starting the 540 epoch\n",
      "Average loss of epoch 541 : Loss 0.00274078\n",
      "starting the 541 epoch\n",
      "Average loss of epoch 542 : Loss 0.00274173\n",
      "starting the 542 epoch\n",
      "Average loss of epoch 543 : Loss 0.00274143\n",
      "starting the 543 epoch\n",
      "Average loss of epoch 544 : Loss 0.00274132\n",
      "starting the 544 epoch\n",
      "Average loss of epoch 545 : Loss 0.00274122\n",
      "starting the 545 epoch\n",
      "Average loss of epoch 546 : Loss 0.00274103\n",
      "starting the 546 epoch\n",
      "Average loss of epoch 547 : Loss 0.00274052\n",
      "starting the 547 epoch\n",
      "Average loss of epoch 548 : Loss 0.00274119\n",
      "starting the 548 epoch\n",
      "Average loss of epoch 549 : Loss 0.00274070\n",
      "starting the 549 epoch\n",
      "Average loss of epoch 550 : Loss 0.00274052\n",
      "starting the 550 epoch\n",
      "Average loss of epoch 551 : Loss 0.00274071\n",
      "starting the 551 epoch\n",
      "Average loss of epoch 552 : Loss 0.00274095\n",
      "starting the 552 epoch\n",
      "Average loss of epoch 553 : Loss 0.00274142\n",
      "starting the 553 epoch\n",
      "Average loss of epoch 554 : Loss 0.00274066\n",
      "starting the 554 epoch\n",
      "Average loss of epoch 555 : Loss 0.00274054\n",
      "starting the 555 epoch\n",
      "Average loss of epoch 556 : Loss 0.00273977\n",
      "starting the 556 epoch\n",
      "Average loss of epoch 557 : Loss 0.00274022\n",
      "starting the 557 epoch\n",
      "Average loss of epoch 558 : Loss 0.00274067\n",
      "starting the 558 epoch\n",
      "Average loss of epoch 559 : Loss 0.00274098\n",
      "starting the 559 epoch\n",
      "Average loss of epoch 560 : Loss 0.00274048\n",
      "starting the 560 epoch\n",
      "Average loss of epoch 561 : Loss 0.00273963\n",
      "starting the 561 epoch\n",
      "Average loss of epoch 562 : Loss 0.00273922\n",
      "starting the 562 epoch\n",
      "Average loss of epoch 563 : Loss 0.00273978\n",
      "starting the 563 epoch\n",
      "Average loss of epoch 564 : Loss 0.00274016\n",
      "starting the 564 epoch\n",
      "Average loss of epoch 565 : Loss 0.00273965\n",
      "starting the 565 epoch\n",
      "Average loss of epoch 566 : Loss 0.00273932\n",
      "starting the 566 epoch\n",
      "Average loss of epoch 567 : Loss 0.00273904\n",
      "starting the 567 epoch\n",
      "Average loss of epoch 568 : Loss 0.00273968\n",
      "starting the 568 epoch\n",
      "Average loss of epoch 569 : Loss 0.00273964\n",
      "starting the 569 epoch\n",
      "Average loss of epoch 570 : Loss 0.00273938\n",
      "starting the 570 epoch\n",
      "Average loss of epoch 571 : Loss 0.00273910\n",
      "starting the 571 epoch\n",
      "Average loss of epoch 572 : Loss 0.00273989\n",
      "starting the 572 epoch\n",
      "Average loss of epoch 573 : Loss 0.00273930\n",
      "starting the 573 epoch\n",
      "Average loss of epoch 574 : Loss 0.00273894\n",
      "starting the 574 epoch\n",
      "Average loss of epoch 575 : Loss 0.00273902\n",
      "starting the 575 epoch\n",
      "Average loss of epoch 576 : Loss 0.00273981\n",
      "starting the 576 epoch\n",
      "Average loss of epoch 577 : Loss 0.00273987\n",
      "starting the 577 epoch\n",
      "Average loss of epoch 578 : Loss 0.00273901\n",
      "starting the 578 epoch\n",
      "Average loss of epoch 579 : Loss 0.00273861\n",
      "starting the 579 epoch\n",
      "Average loss of epoch 580 : Loss 0.00273857\n",
      "starting the 580 epoch\n",
      "Average loss of epoch 581 : Loss 0.00273838\n",
      "starting the 581 epoch\n",
      "Average loss of epoch 582 : Loss 0.00273853\n",
      "starting the 582 epoch\n",
      "Average loss of epoch 583 : Loss 0.00273863\n",
      "starting the 583 epoch\n",
      "Average loss of epoch 584 : Loss 0.00273831\n",
      "starting the 584 epoch\n",
      "Average loss of epoch 585 : Loss 0.00273810\n",
      "starting the 585 epoch\n",
      "Average loss of epoch 586 : Loss 0.00273863\n",
      "starting the 586 epoch\n",
      "Average loss of epoch 587 : Loss 0.00273796\n",
      "starting the 587 epoch\n",
      "Average loss of epoch 588 : Loss 0.00273815\n",
      "starting the 588 epoch\n",
      "Average loss of epoch 589 : Loss 0.00273807\n",
      "starting the 589 epoch\n",
      "Average loss of epoch 590 : Loss 0.00273818\n",
      "starting the 590 epoch\n",
      "Average loss of epoch 591 : Loss 0.00273749\n",
      "starting the 591 epoch\n",
      "Average loss of epoch 592 : Loss 0.00273849\n",
      "starting the 592 epoch\n",
      "Average loss of epoch 593 : Loss 0.00273750\n",
      "starting the 593 epoch\n",
      "Average loss of epoch 594 : Loss 0.00273779\n",
      "starting the 594 epoch\n",
      "Average loss of epoch 595 : Loss 0.00273663\n",
      "starting the 595 epoch\n",
      "Average loss of epoch 596 : Loss 0.00273720\n",
      "starting the 596 epoch\n",
      "Average loss of epoch 597 : Loss 0.00273767\n",
      "starting the 597 epoch\n",
      "Average loss of epoch 598 : Loss 0.00273773\n",
      "starting the 598 epoch\n",
      "Average loss of epoch 599 : Loss 0.00273778\n",
      "starting the 599 epoch\n",
      "Average loss of epoch 600 : Loss 0.00273708\n",
      "starting the 600 epoch\n",
      "Average loss of epoch 601 : Loss 0.00273686\n",
      "starting the 601 epoch\n",
      "Average loss of epoch 602 : Loss 0.00273671\n",
      "starting the 602 epoch\n",
      "Average loss of epoch 603 : Loss 0.00273672\n",
      "starting the 603 epoch\n",
      "Average loss of epoch 604 : Loss 0.00273760\n",
      "starting the 604 epoch\n",
      "Average loss of epoch 605 : Loss 0.00273785\n",
      "starting the 605 epoch\n",
      "Average loss of epoch 606 : Loss 0.00273704\n",
      "starting the 606 epoch\n",
      "Average loss of epoch 607 : Loss 0.00273726\n",
      "starting the 607 epoch\n",
      "Average loss of epoch 608 : Loss 0.00273660\n",
      "starting the 608 epoch\n",
      "Average loss of epoch 609 : Loss 0.00273678\n",
      "starting the 609 epoch\n",
      "Average loss of epoch 610 : Loss 0.00273735\n",
      "starting the 610 epoch\n",
      "Average loss of epoch 611 : Loss 0.00273641\n",
      "starting the 611 epoch\n",
      "Average loss of epoch 612 : Loss 0.00273548\n",
      "starting the 612 epoch\n",
      "Average loss of epoch 613 : Loss 0.00273596\n",
      "starting the 613 epoch\n",
      "Average loss of epoch 614 : Loss 0.00273584\n",
      "starting the 614 epoch\n",
      "Average loss of epoch 615 : Loss 0.00273669\n",
      "starting the 615 epoch\n",
      "Average loss of epoch 616 : Loss 0.00273554\n",
      "starting the 616 epoch\n",
      "Average loss of epoch 617 : Loss 0.00273643\n",
      "starting the 617 epoch\n",
      "Average loss of epoch 618 : Loss 0.00273630\n",
      "starting the 618 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 619 : Loss 0.00273715\n",
      "starting the 619 epoch\n",
      "Average loss of epoch 620 : Loss 0.00273663\n",
      "starting the 620 epoch\n",
      "Average loss of epoch 621 : Loss 0.00273554\n",
      "starting the 621 epoch\n",
      "Average loss of epoch 622 : Loss 0.00273594\n",
      "starting the 622 epoch\n",
      "Average loss of epoch 623 : Loss 0.00273558\n",
      "starting the 623 epoch\n",
      "Average loss of epoch 624 : Loss 0.00273543\n",
      "starting the 624 epoch\n",
      "Average loss of epoch 625 : Loss 0.00273512\n",
      "starting the 625 epoch\n",
      "Average loss of epoch 626 : Loss 0.00273516\n",
      "starting the 626 epoch\n",
      "Average loss of epoch 627 : Loss 0.00273625\n",
      "starting the 627 epoch\n",
      "Average loss of epoch 628 : Loss 0.00273512\n",
      "starting the 628 epoch\n",
      "Average loss of epoch 629 : Loss 0.00273635\n",
      "starting the 629 epoch\n",
      "Average loss of epoch 630 : Loss 0.00273512\n",
      "starting the 630 epoch\n",
      "Average loss of epoch 631 : Loss 0.00273549\n",
      "starting the 631 epoch\n",
      "Average loss of epoch 632 : Loss 0.00273536\n",
      "starting the 632 epoch\n",
      "Average loss of epoch 633 : Loss 0.00273490\n",
      "starting the 633 epoch\n",
      "Average loss of epoch 634 : Loss 0.00273500\n",
      "starting the 634 epoch\n",
      "Average loss of epoch 635 : Loss 0.00273541\n",
      "starting the 635 epoch\n",
      "Average loss of epoch 636 : Loss 0.00273544\n",
      "starting the 636 epoch\n",
      "Average loss of epoch 637 : Loss 0.00273497\n",
      "starting the 637 epoch\n",
      "Average loss of epoch 638 : Loss 0.00273445\n",
      "starting the 638 epoch\n",
      "Average loss of epoch 639 : Loss 0.00273413\n",
      "starting the 639 epoch\n",
      "Average loss of epoch 640 : Loss 0.00273460\n",
      "starting the 640 epoch\n",
      "Average loss of epoch 641 : Loss 0.00273503\n",
      "starting the 641 epoch\n",
      "Average loss of epoch 642 : Loss 0.00273451\n",
      "starting the 642 epoch\n",
      "Average loss of epoch 643 : Loss 0.00273466\n",
      "starting the 643 epoch\n",
      "Average loss of epoch 644 : Loss 0.00273403\n",
      "starting the 644 epoch\n",
      "Average loss of epoch 645 : Loss 0.00273432\n",
      "starting the 645 epoch\n",
      "Average loss of epoch 646 : Loss 0.00273403\n",
      "starting the 646 epoch\n",
      "Average loss of epoch 647 : Loss 0.00273438\n",
      "starting the 647 epoch\n",
      "Average loss of epoch 648 : Loss 0.00273390\n",
      "starting the 648 epoch\n",
      "Average loss of epoch 649 : Loss 0.00273389\n",
      "starting the 649 epoch\n",
      "Average loss of epoch 650 : Loss 0.00273532\n",
      "starting the 650 epoch\n",
      "Average loss of epoch 651 : Loss 0.00273317\n",
      "starting the 651 epoch\n",
      "Average loss of epoch 652 : Loss 0.00273391\n",
      "starting the 652 epoch\n",
      "Average loss of epoch 653 : Loss 0.00273374\n",
      "starting the 653 epoch\n",
      "Average loss of epoch 654 : Loss 0.00273372\n",
      "starting the 654 epoch\n",
      "Average loss of epoch 655 : Loss 0.00273320\n",
      "starting the 655 epoch\n",
      "Average loss of epoch 656 : Loss 0.00273364\n",
      "starting the 656 epoch\n",
      "Average loss of epoch 657 : Loss 0.00273337\n",
      "starting the 657 epoch\n",
      "Average loss of epoch 658 : Loss 0.00273301\n",
      "starting the 658 epoch\n",
      "Average loss of epoch 659 : Loss 0.00273276\n",
      "starting the 659 epoch\n",
      "Average loss of epoch 660 : Loss 0.00273294\n",
      "starting the 660 epoch\n",
      "Average loss of epoch 661 : Loss 0.00273249\n",
      "starting the 661 epoch\n",
      "Average loss of epoch 662 : Loss 0.00273277\n",
      "starting the 662 epoch\n",
      "Average loss of epoch 663 : Loss 0.00273306\n",
      "starting the 663 epoch\n",
      "Average loss of epoch 664 : Loss 0.00273249\n",
      "starting the 664 epoch\n",
      "Average loss of epoch 665 : Loss 0.00273303\n",
      "starting the 665 epoch\n",
      "Average loss of epoch 666 : Loss 0.00273262\n",
      "starting the 666 epoch\n",
      "Average loss of epoch 667 : Loss 0.00273260\n",
      "starting the 667 epoch\n",
      "Average loss of epoch 668 : Loss 0.00273285\n",
      "starting the 668 epoch\n",
      "Average loss of epoch 669 : Loss 0.00273299\n",
      "starting the 669 epoch\n",
      "Average loss of epoch 670 : Loss 0.00273278\n",
      "starting the 670 epoch\n",
      "Average loss of epoch 671 : Loss 0.00273232\n",
      "starting the 671 epoch\n",
      "Average loss of epoch 672 : Loss 0.00273218\n",
      "starting the 672 epoch\n",
      "Average loss of epoch 673 : Loss 0.00273205\n",
      "starting the 673 epoch\n",
      "Average loss of epoch 674 : Loss 0.00273208\n",
      "starting the 674 epoch\n",
      "Average loss of epoch 675 : Loss 0.00273183\n",
      "starting the 675 epoch\n",
      "Average loss of epoch 676 : Loss 0.00273264\n",
      "starting the 676 epoch\n",
      "Average loss of epoch 677 : Loss 0.00273194\n",
      "starting the 677 epoch\n",
      "Average loss of epoch 678 : Loss 0.00273213\n",
      "starting the 678 epoch\n",
      "Average loss of epoch 679 : Loss 0.00273222\n",
      "starting the 679 epoch\n",
      "Average loss of epoch 680 : Loss 0.00273152\n",
      "starting the 680 epoch\n",
      "Average loss of epoch 681 : Loss 0.00273154\n",
      "starting the 681 epoch\n",
      "Average loss of epoch 682 : Loss 0.00273071\n",
      "starting the 682 epoch\n",
      "Average loss of epoch 683 : Loss 0.00273211\n",
      "starting the 683 epoch\n",
      "Average loss of epoch 684 : Loss 0.00273142\n",
      "starting the 684 epoch\n",
      "Average loss of epoch 685 : Loss 0.00273212\n",
      "starting the 685 epoch\n",
      "Average loss of epoch 686 : Loss 0.00273086\n",
      "starting the 686 epoch\n",
      "Average loss of epoch 687 : Loss 0.00273082\n",
      "starting the 687 epoch\n",
      "Average loss of epoch 688 : Loss 0.00273036\n",
      "starting the 688 epoch\n",
      "Average loss of epoch 689 : Loss 0.00273107\n",
      "starting the 689 epoch\n",
      "Average loss of epoch 690 : Loss 0.00273103\n",
      "starting the 690 epoch\n",
      "Average loss of epoch 691 : Loss 0.00273082\n",
      "starting the 691 epoch\n",
      "Average loss of epoch 692 : Loss 0.00273144\n",
      "starting the 692 epoch\n",
      "Average loss of epoch 693 : Loss 0.00273097\n",
      "starting the 693 epoch\n",
      "Average loss of epoch 694 : Loss 0.00273101\n",
      "starting the 694 epoch\n",
      "Average loss of epoch 695 : Loss 0.00273038\n",
      "starting the 695 epoch\n",
      "Average loss of epoch 696 : Loss 0.00273037\n",
      "starting the 696 epoch\n",
      "Average loss of epoch 697 : Loss 0.00273066\n",
      "starting the 697 epoch\n",
      "Average loss of epoch 698 : Loss 0.00273143\n",
      "starting the 698 epoch\n",
      "Average loss of epoch 699 : Loss 0.00273116\n",
      "starting the 699 epoch\n",
      "Average loss of epoch 700 : Loss 0.00273013\n",
      "starting the 700 epoch\n",
      "Average loss of epoch 701 : Loss 0.00273035\n",
      "starting the 701 epoch\n",
      "Average loss of epoch 702 : Loss 0.00273125\n",
      "starting the 702 epoch\n",
      "Average loss of epoch 703 : Loss 0.00272966\n",
      "starting the 703 epoch\n",
      "Average loss of epoch 704 : Loss 0.00273022\n",
      "starting the 704 epoch\n",
      "Average loss of epoch 705 : Loss 0.00273070\n",
      "starting the 705 epoch\n",
      "Average loss of epoch 706 : Loss 0.00272967\n",
      "starting the 706 epoch\n",
      "Average loss of epoch 707 : Loss 0.00273003\n",
      "starting the 707 epoch\n",
      "Average loss of epoch 708 : Loss 0.00272898\n",
      "starting the 708 epoch\n",
      "Average loss of epoch 709 : Loss 0.00273009\n",
      "starting the 709 epoch\n",
      "Average loss of epoch 710 : Loss 0.00272916\n",
      "starting the 710 epoch\n",
      "Average loss of epoch 711 : Loss 0.00272951\n",
      "starting the 711 epoch\n",
      "Average loss of epoch 712 : Loss 0.00272941\n",
      "starting the 712 epoch\n",
      "Average loss of epoch 713 : Loss 0.00272973\n",
      "starting the 713 epoch\n",
      "Average loss of epoch 714 : Loss 0.00272973\n",
      "starting the 714 epoch\n",
      "Average loss of epoch 715 : Loss 0.00272847\n",
      "starting the 715 epoch\n",
      "Average loss of epoch 716 : Loss 0.00272923\n",
      "starting the 716 epoch\n",
      "Average loss of epoch 717 : Loss 0.00272918\n",
      "starting the 717 epoch\n",
      "Average loss of epoch 718 : Loss 0.00272928\n",
      "starting the 718 epoch\n",
      "Average loss of epoch 719 : Loss 0.00272915\n",
      "starting the 719 epoch\n",
      "Average loss of epoch 720 : Loss 0.00272859\n",
      "starting the 720 epoch\n",
      "Average loss of epoch 721 : Loss 0.00272925\n",
      "starting the 721 epoch\n",
      "Average loss of epoch 722 : Loss 0.00272897\n",
      "starting the 722 epoch\n",
      "Average loss of epoch 723 : Loss 0.00272821\n",
      "starting the 723 epoch\n",
      "Average loss of epoch 724 : Loss 0.00272831\n",
      "starting the 724 epoch\n",
      "Average loss of epoch 725 : Loss 0.00272804\n",
      "starting the 725 epoch\n",
      "Average loss of epoch 726 : Loss 0.00272828\n",
      "starting the 726 epoch\n",
      "Average loss of epoch 727 : Loss 0.00272911\n",
      "starting the 727 epoch\n",
      "Average loss of epoch 728 : Loss 0.00272835\n",
      "starting the 728 epoch\n",
      "Average loss of epoch 729 : Loss 0.00272827\n",
      "starting the 729 epoch\n",
      "Average loss of epoch 730 : Loss 0.00272890\n",
      "starting the 730 epoch\n",
      "Average loss of epoch 731 : Loss 0.00272731\n",
      "starting the 731 epoch\n",
      "Average loss of epoch 732 : Loss 0.00272906\n",
      "starting the 732 epoch\n",
      "Average loss of epoch 733 : Loss 0.00272778\n",
      "starting the 733 epoch\n",
      "Average loss of epoch 734 : Loss 0.00272734\n",
      "starting the 734 epoch\n",
      "Average loss of epoch 735 : Loss 0.00272809\n",
      "starting the 735 epoch\n",
      "Average loss of epoch 736 : Loss 0.00272837\n",
      "starting the 736 epoch\n",
      "Average loss of epoch 737 : Loss 0.00272805\n",
      "starting the 737 epoch\n",
      "Average loss of epoch 738 : Loss 0.00272739\n",
      "starting the 738 epoch\n",
      "Average loss of epoch 739 : Loss 0.00272783\n",
      "starting the 739 epoch\n",
      "Average loss of epoch 740 : Loss 0.00272739\n",
      "starting the 740 epoch\n",
      "Average loss of epoch 741 : Loss 0.00272796\n",
      "starting the 741 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 742 : Loss 0.00272723\n",
      "starting the 742 epoch\n",
      "Average loss of epoch 743 : Loss 0.00272690\n",
      "starting the 743 epoch\n",
      "Average loss of epoch 744 : Loss 0.00272743\n",
      "starting the 744 epoch\n",
      "Average loss of epoch 745 : Loss 0.00272737\n",
      "starting the 745 epoch\n",
      "Average loss of epoch 746 : Loss 0.00272715\n",
      "starting the 746 epoch\n",
      "Average loss of epoch 747 : Loss 0.00272624\n",
      "starting the 747 epoch\n",
      "Average loss of epoch 748 : Loss 0.00272680\n",
      "starting the 748 epoch\n",
      "Average loss of epoch 749 : Loss 0.00272752\n",
      "starting the 749 epoch\n",
      "Average loss of epoch 750 : Loss 0.00272769\n",
      "starting the 750 epoch\n",
      "Average loss of epoch 751 : Loss 0.00272735\n",
      "starting the 751 epoch\n",
      "Average loss of epoch 752 : Loss 0.00272646\n",
      "starting the 752 epoch\n",
      "Average loss of epoch 753 : Loss 0.00272731\n",
      "starting the 753 epoch\n",
      "Average loss of epoch 754 : Loss 0.00272734\n",
      "starting the 754 epoch\n",
      "Average loss of epoch 755 : Loss 0.00272633\n",
      "starting the 755 epoch\n",
      "Average loss of epoch 756 : Loss 0.00272682\n",
      "starting the 756 epoch\n",
      "Average loss of epoch 757 : Loss 0.00272656\n",
      "starting the 757 epoch\n",
      "Average loss of epoch 758 : Loss 0.00272612\n",
      "starting the 758 epoch\n",
      "Average loss of epoch 759 : Loss 0.00272611\n",
      "starting the 759 epoch\n",
      "Average loss of epoch 760 : Loss 0.00272725\n",
      "starting the 760 epoch\n",
      "Average loss of epoch 761 : Loss 0.00272632\n",
      "starting the 761 epoch\n",
      "Average loss of epoch 762 : Loss 0.00272659\n",
      "starting the 762 epoch\n",
      "Average loss of epoch 763 : Loss 0.00272630\n",
      "starting the 763 epoch\n",
      "Average loss of epoch 764 : Loss 0.00272734\n",
      "starting the 764 epoch\n",
      "Average loss of epoch 765 : Loss 0.00272554\n",
      "starting the 765 epoch\n",
      "Average loss of epoch 766 : Loss 0.00272535\n",
      "starting the 766 epoch\n",
      "Average loss of epoch 767 : Loss 0.00272532\n",
      "starting the 767 epoch\n",
      "Average loss of epoch 768 : Loss 0.00272558\n",
      "starting the 768 epoch\n",
      "Average loss of epoch 769 : Loss 0.00272578\n",
      "starting the 769 epoch\n",
      "Average loss of epoch 770 : Loss 0.00272548\n",
      "starting the 770 epoch\n",
      "Average loss of epoch 771 : Loss 0.00272472\n",
      "starting the 771 epoch\n",
      "Average loss of epoch 772 : Loss 0.00272514\n",
      "starting the 772 epoch\n",
      "Average loss of epoch 773 : Loss 0.00272594\n",
      "starting the 773 epoch\n",
      "Average loss of epoch 774 : Loss 0.00272572\n",
      "starting the 774 epoch\n",
      "Average loss of epoch 775 : Loss 0.00272494\n",
      "starting the 775 epoch\n",
      "Average loss of epoch 776 : Loss 0.00272527\n",
      "starting the 776 epoch\n",
      "Average loss of epoch 777 : Loss 0.00272478\n",
      "starting the 777 epoch\n",
      "Average loss of epoch 778 : Loss 0.00272472\n",
      "starting the 778 epoch\n",
      "Average loss of epoch 779 : Loss 0.00272520\n",
      "starting the 779 epoch\n",
      "Average loss of epoch 780 : Loss 0.00272527\n",
      "starting the 780 epoch\n",
      "Average loss of epoch 781 : Loss 0.00272376\n",
      "starting the 781 epoch\n",
      "Average loss of epoch 782 : Loss 0.00272536\n",
      "starting the 782 epoch\n",
      "Average loss of epoch 783 : Loss 0.00272459\n",
      "starting the 783 epoch\n",
      "Average loss of epoch 784 : Loss 0.00272418\n",
      "starting the 784 epoch\n",
      "Average loss of epoch 785 : Loss 0.00272424\n",
      "starting the 785 epoch\n",
      "Average loss of epoch 786 : Loss 0.00272382\n",
      "starting the 786 epoch\n",
      "Average loss of epoch 787 : Loss 0.00272426\n",
      "starting the 787 epoch\n",
      "Average loss of epoch 788 : Loss 0.00272493\n",
      "starting the 788 epoch\n",
      "Average loss of epoch 789 : Loss 0.00272392\n",
      "starting the 789 epoch\n",
      "Average loss of epoch 790 : Loss 0.00272413\n",
      "starting the 790 epoch\n",
      "Average loss of epoch 791 : Loss 0.00272365\n",
      "starting the 791 epoch\n",
      "Average loss of epoch 792 : Loss 0.00272359\n",
      "starting the 792 epoch\n",
      "Average loss of epoch 793 : Loss 0.00272324\n",
      "starting the 793 epoch\n",
      "Average loss of epoch 794 : Loss 0.00272338\n",
      "starting the 794 epoch\n",
      "Average loss of epoch 795 : Loss 0.00272489\n",
      "starting the 795 epoch\n",
      "Average loss of epoch 796 : Loss 0.00272392\n",
      "starting the 796 epoch\n",
      "Average loss of epoch 797 : Loss 0.00272346\n",
      "starting the 797 epoch\n",
      "Average loss of epoch 798 : Loss 0.00272302\n",
      "starting the 798 epoch\n",
      "Average loss of epoch 799 : Loss 0.00272368\n",
      "starting the 799 epoch\n",
      "Average loss of epoch 800 : Loss 0.00272344\n",
      "starting the 800 epoch\n",
      "Average loss of epoch 801 : Loss 0.00272303\n",
      "starting the 801 epoch\n",
      "Average loss of epoch 802 : Loss 0.00272302\n",
      "starting the 802 epoch\n",
      "Average loss of epoch 803 : Loss 0.00272318\n",
      "starting the 803 epoch\n",
      "Average loss of epoch 804 : Loss 0.00272323\n",
      "starting the 804 epoch\n",
      "Average loss of epoch 805 : Loss 0.00272264\n",
      "starting the 805 epoch\n",
      "Average loss of epoch 806 : Loss 0.00272248\n",
      "starting the 806 epoch\n",
      "Average loss of epoch 807 : Loss 0.00272335\n",
      "starting the 807 epoch\n",
      "Average loss of epoch 808 : Loss 0.00272329\n",
      "starting the 808 epoch\n",
      "Average loss of epoch 809 : Loss 0.00272370\n",
      "starting the 809 epoch\n",
      "Average loss of epoch 810 : Loss 0.00272304\n",
      "starting the 810 epoch\n",
      "Average loss of epoch 811 : Loss 0.00272250\n",
      "starting the 811 epoch\n",
      "Average loss of epoch 812 : Loss 0.00272201\n",
      "starting the 812 epoch\n",
      "Average loss of epoch 813 : Loss 0.00272278\n",
      "starting the 813 epoch\n",
      "Average loss of epoch 814 : Loss 0.00272213\n",
      "starting the 814 epoch\n",
      "Average loss of epoch 815 : Loss 0.00272241\n",
      "starting the 815 epoch\n",
      "Average loss of epoch 816 : Loss 0.00272263\n",
      "starting the 816 epoch\n",
      "Average loss of epoch 817 : Loss 0.00272288\n",
      "starting the 817 epoch\n",
      "Average loss of epoch 818 : Loss 0.00272268\n",
      "starting the 818 epoch\n",
      "Average loss of epoch 819 : Loss 0.00272194\n",
      "starting the 819 epoch\n",
      "Average loss of epoch 820 : Loss 0.00272243\n",
      "starting the 820 epoch\n",
      "Average loss of epoch 821 : Loss 0.00272155\n",
      "starting the 821 epoch\n",
      "Average loss of epoch 822 : Loss 0.00272214\n",
      "starting the 822 epoch\n",
      "Average loss of epoch 823 : Loss 0.00272144\n",
      "starting the 823 epoch\n",
      "Average loss of epoch 824 : Loss 0.00272195\n",
      "starting the 824 epoch\n",
      "Average loss of epoch 825 : Loss 0.00272244\n",
      "starting the 825 epoch\n",
      "Average loss of epoch 826 : Loss 0.00272241\n",
      "starting the 826 epoch\n",
      "Average loss of epoch 827 : Loss 0.00272157\n",
      "starting the 827 epoch\n",
      "Average loss of epoch 828 : Loss 0.00272157\n",
      "starting the 828 epoch\n",
      "Average loss of epoch 829 : Loss 0.00272082\n",
      "starting the 829 epoch\n",
      "Average loss of epoch 830 : Loss 0.00272154\n",
      "starting the 830 epoch\n",
      "Average loss of epoch 831 : Loss 0.00272190\n",
      "starting the 831 epoch\n",
      "Average loss of epoch 832 : Loss 0.00272148\n",
      "starting the 832 epoch\n",
      "Average loss of epoch 833 : Loss 0.00272146\n",
      "starting the 833 epoch\n",
      "Average loss of epoch 834 : Loss 0.00272090\n",
      "starting the 834 epoch\n",
      "Average loss of epoch 835 : Loss 0.00272111\n",
      "starting the 835 epoch\n",
      "Average loss of epoch 836 : Loss 0.00272122\n",
      "starting the 836 epoch\n",
      "Average loss of epoch 837 : Loss 0.00272083\n",
      "starting the 837 epoch\n",
      "Average loss of epoch 838 : Loss 0.00272035\n",
      "starting the 838 epoch\n",
      "Average loss of epoch 839 : Loss 0.00272097\n",
      "starting the 839 epoch\n",
      "Average loss of epoch 840 : Loss 0.00272141\n",
      "starting the 840 epoch\n",
      "Average loss of epoch 841 : Loss 0.00272071\n",
      "starting the 841 epoch\n",
      "Average loss of epoch 842 : Loss 0.00272095\n",
      "starting the 842 epoch\n",
      "Average loss of epoch 843 : Loss 0.00272133\n",
      "starting the 843 epoch\n",
      "Average loss of epoch 844 : Loss 0.00272014\n",
      "starting the 844 epoch\n",
      "Average loss of epoch 845 : Loss 0.00272050\n",
      "starting the 845 epoch\n",
      "Average loss of epoch 846 : Loss 0.00272019\n",
      "starting the 846 epoch\n",
      "Average loss of epoch 847 : Loss 0.00272077\n",
      "starting the 847 epoch\n",
      "Average loss of epoch 848 : Loss 0.00271946\n",
      "starting the 848 epoch\n",
      "Average loss of epoch 849 : Loss 0.00271962\n",
      "starting the 849 epoch\n",
      "Average loss of epoch 850 : Loss 0.00271961\n",
      "starting the 850 epoch\n",
      "Average loss of epoch 851 : Loss 0.00272023\n",
      "starting the 851 epoch\n",
      "Average loss of epoch 852 : Loss 0.00271916\n",
      "starting the 852 epoch\n",
      "Average loss of epoch 853 : Loss 0.00271930\n",
      "starting the 853 epoch\n",
      "Average loss of epoch 854 : Loss 0.00272018\n",
      "starting the 854 epoch\n",
      "Average loss of epoch 855 : Loss 0.00271942\n",
      "starting the 855 epoch\n",
      "Average loss of epoch 856 : Loss 0.00272016\n",
      "starting the 856 epoch\n",
      "Average loss of epoch 857 : Loss 0.00271995\n",
      "starting the 857 epoch\n",
      "Average loss of epoch 858 : Loss 0.00271881\n",
      "starting the 858 epoch\n",
      "Average loss of epoch 859 : Loss 0.00271961\n",
      "starting the 859 epoch\n",
      "Average loss of epoch 860 : Loss 0.00271946\n",
      "starting the 860 epoch\n",
      "Average loss of epoch 861 : Loss 0.00271911\n",
      "starting the 861 epoch\n",
      "Average loss of epoch 862 : Loss 0.00271931\n",
      "starting the 862 epoch\n",
      "Average loss of epoch 863 : Loss 0.00271910\n",
      "starting the 863 epoch\n",
      "Average loss of epoch 864 : Loss 0.00271857\n",
      "starting the 864 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 865 : Loss 0.00271870\n",
      "starting the 865 epoch\n",
      "Average loss of epoch 866 : Loss 0.00271775\n",
      "starting the 866 epoch\n",
      "Average loss of epoch 867 : Loss 0.00271829\n",
      "starting the 867 epoch\n",
      "Average loss of epoch 868 : Loss 0.00271867\n",
      "starting the 868 epoch\n",
      "Average loss of epoch 869 : Loss 0.00271860\n",
      "starting the 869 epoch\n",
      "Average loss of epoch 870 : Loss 0.00271830\n",
      "starting the 870 epoch\n",
      "Average loss of epoch 871 : Loss 0.00271926\n",
      "starting the 871 epoch\n",
      "Average loss of epoch 872 : Loss 0.00271792\n",
      "starting the 872 epoch\n",
      "Average loss of epoch 873 : Loss 0.00271794\n",
      "starting the 873 epoch\n",
      "Average loss of epoch 874 : Loss 0.00271828\n",
      "starting the 874 epoch\n",
      "Average loss of epoch 875 : Loss 0.00271819\n",
      "starting the 875 epoch\n",
      "Average loss of epoch 876 : Loss 0.00271748\n",
      "starting the 876 epoch\n",
      "Average loss of epoch 877 : Loss 0.00271862\n",
      "starting the 877 epoch\n",
      "Average loss of epoch 878 : Loss 0.00271761\n",
      "starting the 878 epoch\n",
      "Average loss of epoch 879 : Loss 0.00271771\n",
      "starting the 879 epoch\n",
      "Average loss of epoch 880 : Loss 0.00271840\n",
      "starting the 880 epoch\n",
      "Average loss of epoch 881 : Loss 0.00271721\n",
      "starting the 881 epoch\n",
      "Average loss of epoch 882 : Loss 0.00271770\n",
      "starting the 882 epoch\n",
      "Average loss of epoch 883 : Loss 0.00271864\n",
      "starting the 883 epoch\n",
      "Average loss of epoch 884 : Loss 0.00271795\n",
      "starting the 884 epoch\n",
      "Average loss of epoch 885 : Loss 0.00271753\n",
      "starting the 885 epoch\n",
      "Average loss of epoch 886 : Loss 0.00271740\n",
      "starting the 886 epoch\n",
      "Average loss of epoch 887 : Loss 0.00271725\n",
      "starting the 887 epoch\n",
      "Average loss of epoch 888 : Loss 0.00271730\n",
      "starting the 888 epoch\n",
      "Average loss of epoch 889 : Loss 0.00271730\n",
      "starting the 889 epoch\n",
      "Average loss of epoch 890 : Loss 0.00271728\n",
      "starting the 890 epoch\n",
      "Average loss of epoch 891 : Loss 0.00271710\n",
      "starting the 891 epoch\n",
      "Average loss of epoch 892 : Loss 0.00271694\n",
      "starting the 892 epoch\n",
      "Average loss of epoch 893 : Loss 0.00271678\n",
      "starting the 893 epoch\n",
      "Average loss of epoch 894 : Loss 0.00271689\n",
      "starting the 894 epoch\n",
      "Average loss of epoch 895 : Loss 0.00271735\n",
      "starting the 895 epoch\n",
      "Average loss of epoch 896 : Loss 0.00271656\n",
      "starting the 896 epoch\n",
      "Average loss of epoch 897 : Loss 0.00271652\n",
      "starting the 897 epoch\n",
      "Average loss of epoch 898 : Loss 0.00271597\n",
      "starting the 898 epoch\n",
      "Average loss of epoch 899 : Loss 0.00271637\n",
      "starting the 899 epoch\n",
      "Average loss of epoch 900 : Loss 0.00271653\n",
      "starting the 900 epoch\n",
      "Average loss of epoch 901 : Loss 0.00271676\n",
      "starting the 901 epoch\n",
      "Average loss of epoch 902 : Loss 0.00271598\n",
      "starting the 902 epoch\n",
      "Average loss of epoch 903 : Loss 0.00271631\n",
      "starting the 903 epoch\n",
      "Average loss of epoch 904 : Loss 0.00271692\n",
      "starting the 904 epoch\n",
      "Average loss of epoch 905 : Loss 0.00271714\n",
      "starting the 905 epoch\n",
      "Average loss of epoch 906 : Loss 0.00271630\n",
      "starting the 906 epoch\n",
      "Average loss of epoch 907 : Loss 0.00271558\n",
      "starting the 907 epoch\n",
      "Average loss of epoch 908 : Loss 0.00271580\n",
      "starting the 908 epoch\n",
      "Average loss of epoch 909 : Loss 0.00271649\n",
      "starting the 909 epoch\n",
      "Average loss of epoch 910 : Loss 0.00271553\n",
      "starting the 910 epoch\n",
      "Average loss of epoch 911 : Loss 0.00271499\n",
      "starting the 911 epoch\n",
      "Average loss of epoch 912 : Loss 0.00271612\n",
      "starting the 912 epoch\n",
      "Average loss of epoch 913 : Loss 0.00271582\n",
      "starting the 913 epoch\n",
      "Average loss of epoch 914 : Loss 0.00271560\n",
      "starting the 914 epoch\n",
      "Average loss of epoch 915 : Loss 0.00271488\n",
      "starting the 915 epoch\n",
      "Average loss of epoch 916 : Loss 0.00271521\n",
      "starting the 916 epoch\n",
      "Average loss of epoch 917 : Loss 0.00271527\n",
      "starting the 917 epoch\n",
      "Average loss of epoch 918 : Loss 0.00271571\n",
      "starting the 918 epoch\n",
      "Average loss of epoch 919 : Loss 0.00271593\n",
      "starting the 919 epoch\n",
      "Average loss of epoch 920 : Loss 0.00271493\n",
      "starting the 920 epoch\n",
      "Average loss of epoch 921 : Loss 0.00271494\n",
      "starting the 921 epoch\n",
      "Average loss of epoch 922 : Loss 0.00271511\n",
      "starting the 922 epoch\n",
      "Average loss of epoch 923 : Loss 0.00271435\n",
      "starting the 923 epoch\n",
      "Average loss of epoch 924 : Loss 0.00271472\n",
      "starting the 924 epoch\n",
      "Average loss of epoch 925 : Loss 0.00271504\n",
      "starting the 925 epoch\n",
      "Average loss of epoch 926 : Loss 0.00271556\n",
      "starting the 926 epoch\n",
      "Average loss of epoch 927 : Loss 0.00271479\n",
      "starting the 927 epoch\n",
      "Average loss of epoch 928 : Loss 0.00271443\n",
      "starting the 928 epoch\n",
      "Average loss of epoch 929 : Loss 0.00271419\n",
      "starting the 929 epoch\n",
      "Average loss of epoch 930 : Loss 0.00271435\n",
      "starting the 930 epoch\n",
      "Average loss of epoch 931 : Loss 0.00271468\n",
      "starting the 931 epoch\n",
      "Average loss of epoch 932 : Loss 0.00271419\n",
      "starting the 932 epoch\n",
      "Average loss of epoch 933 : Loss 0.00271446\n",
      "starting the 933 epoch\n",
      "Average loss of epoch 934 : Loss 0.00271458\n",
      "starting the 934 epoch\n",
      "Average loss of epoch 935 : Loss 0.00271363\n",
      "starting the 935 epoch\n",
      "Average loss of epoch 936 : Loss 0.00271417\n",
      "starting the 936 epoch\n",
      "Average loss of epoch 937 : Loss 0.00271425\n",
      "starting the 937 epoch\n",
      "Average loss of epoch 938 : Loss 0.00271400\n",
      "starting the 938 epoch\n",
      "Average loss of epoch 939 : Loss 0.00271421\n",
      "starting the 939 epoch\n",
      "Average loss of epoch 940 : Loss 0.00271428\n",
      "starting the 940 epoch\n",
      "Average loss of epoch 941 : Loss 0.00271297\n",
      "starting the 941 epoch\n",
      "Average loss of epoch 942 : Loss 0.00271407\n",
      "starting the 942 epoch\n",
      "Average loss of epoch 943 : Loss 0.00271372\n",
      "starting the 943 epoch\n",
      "Average loss of epoch 944 : Loss 0.00271383\n",
      "starting the 944 epoch\n",
      "Average loss of epoch 945 : Loss 0.00271336\n",
      "starting the 945 epoch\n",
      "Average loss of epoch 946 : Loss 0.00271309\n",
      "starting the 946 epoch\n",
      "Average loss of epoch 947 : Loss 0.00271302\n",
      "starting the 947 epoch\n",
      "Average loss of epoch 948 : Loss 0.00271441\n",
      "starting the 948 epoch\n",
      "Average loss of epoch 949 : Loss 0.00271261\n",
      "starting the 949 epoch\n",
      "Average loss of epoch 950 : Loss 0.00271348\n",
      "starting the 950 epoch\n",
      "Average loss of epoch 951 : Loss 0.00271294\n",
      "starting the 951 epoch\n",
      "Average loss of epoch 952 : Loss 0.00271274\n",
      "starting the 952 epoch\n",
      "Average loss of epoch 953 : Loss 0.00271301\n",
      "starting the 953 epoch\n",
      "Average loss of epoch 954 : Loss 0.00271286\n",
      "starting the 954 epoch\n",
      "Average loss of epoch 955 : Loss 0.00271317\n",
      "starting the 955 epoch\n",
      "Average loss of epoch 956 : Loss 0.00271216\n",
      "starting the 956 epoch\n",
      "Average loss of epoch 957 : Loss 0.00271261\n",
      "starting the 957 epoch\n",
      "Average loss of epoch 958 : Loss 0.00271312\n",
      "starting the 958 epoch\n",
      "Average loss of epoch 959 : Loss 0.00271304\n",
      "starting the 959 epoch\n",
      "Average loss of epoch 960 : Loss 0.00271273\n",
      "starting the 960 epoch\n",
      "Average loss of epoch 961 : Loss 0.00271239\n",
      "starting the 961 epoch\n",
      "Average loss of epoch 962 : Loss 0.00271248\n",
      "starting the 962 epoch\n",
      "Average loss of epoch 963 : Loss 0.00271188\n",
      "starting the 963 epoch\n",
      "Average loss of epoch 964 : Loss 0.00271190\n",
      "starting the 964 epoch\n",
      "Average loss of epoch 965 : Loss 0.00271200\n",
      "starting the 965 epoch\n",
      "Average loss of epoch 966 : Loss 0.00271271\n",
      "starting the 966 epoch\n",
      "Average loss of epoch 967 : Loss 0.00271173\n",
      "starting the 967 epoch\n",
      "Average loss of epoch 968 : Loss 0.00271147\n",
      "starting the 968 epoch\n",
      "Average loss of epoch 969 : Loss 0.00271187\n",
      "starting the 969 epoch\n",
      "Average loss of epoch 970 : Loss 0.00271221\n",
      "starting the 970 epoch\n",
      "Average loss of epoch 971 : Loss 0.00271200\n",
      "starting the 971 epoch\n",
      "Average loss of epoch 972 : Loss 0.00271154\n",
      "starting the 972 epoch\n",
      "Average loss of epoch 973 : Loss 0.00271128\n",
      "starting the 973 epoch\n",
      "Average loss of epoch 974 : Loss 0.00271133\n",
      "starting the 974 epoch\n",
      "Average loss of epoch 975 : Loss 0.00271151\n",
      "starting the 975 epoch\n",
      "Average loss of epoch 976 : Loss 0.00271056\n",
      "starting the 976 epoch\n",
      "Average loss of epoch 977 : Loss 0.00271197\n",
      "starting the 977 epoch\n",
      "Average loss of epoch 978 : Loss 0.00271088\n",
      "starting the 978 epoch\n",
      "Average loss of epoch 979 : Loss 0.00271089\n",
      "starting the 979 epoch\n",
      "Average loss of epoch 980 : Loss 0.00271099\n",
      "starting the 980 epoch\n",
      "Average loss of epoch 981 : Loss 0.00271110\n",
      "starting the 981 epoch\n",
      "Average loss of epoch 982 : Loss 0.00271189\n",
      "starting the 982 epoch\n",
      "Average loss of epoch 983 : Loss 0.00271093\n",
      "starting the 983 epoch\n",
      "Average loss of epoch 984 : Loss 0.00271205\n",
      "starting the 984 epoch\n",
      "Average loss of epoch 985 : Loss 0.00271082\n",
      "starting the 985 epoch\n",
      "Average loss of epoch 986 : Loss 0.00271024\n",
      "starting the 986 epoch\n",
      "Average loss of epoch 987 : Loss 0.00271060\n",
      "starting the 987 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of epoch 988 : Loss 0.00271043\n",
      "starting the 988 epoch\n",
      "Average loss of epoch 989 : Loss 0.00271032\n",
      "starting the 989 epoch\n",
      "Average loss of epoch 990 : Loss 0.00271117\n",
      "starting the 990 epoch\n",
      "Average loss of epoch 991 : Loss 0.00271064\n",
      "starting the 991 epoch\n",
      "Average loss of epoch 992 : Loss 0.00271035\n",
      "starting the 992 epoch\n",
      "Average loss of epoch 993 : Loss 0.00271071\n",
      "starting the 993 epoch\n",
      "Average loss of epoch 994 : Loss 0.00271045\n",
      "starting the 994 epoch\n",
      "Average loss of epoch 995 : Loss 0.00270980\n",
      "starting the 995 epoch\n",
      "Average loss of epoch 996 : Loss 0.00270976\n",
      "starting the 996 epoch\n",
      "Average loss of epoch 997 : Loss 0.00271026\n",
      "starting the 997 epoch\n",
      "Average loss of epoch 998 : Loss 0.00271055\n",
      "starting the 998 epoch\n",
      "Average loss of epoch 999 : Loss 0.00270926\n",
      "starting the 999 epoch\n",
      "Average loss of epoch 1000 : Loss 0.00271099\n",
      "Saving the Model\n",
      "Successfully save the model of time.struct_time(tm_year=2020, tm_mon=3, tm_mday=29, tm_hour=17, tm_min=36, tm_sec=9, tm_wday=6, tm_yday=89, tm_isdst=1)\n"
     ]
    }
   ],
   "source": [
    "opt.max_epochs = 1000\n",
    "epoch_loss = torch.zeros(opt.max_epochs)\n",
    "for epoch in range(opt.max_epochs):\n",
    "    print(\"starting the {} epoch\".format(epoch))\n",
    "    for training_data, training_label in train_loader:\n",
    "        if (torch.cuda.is_available()):\n",
    "            training_data = training_data.cuda()\n",
    "            training_label = training_label.cuda()\n",
    "        predict_label = net(training_data)\n",
    "        training_label = training_label.float()\n",
    "        predict_label = predict_label.view(predict_label.shape[0], predict_label.shape[2], -1)\n",
    "        #print(training_data.shape)\n",
    "        #print(predict_label.shape)\n",
    "        #print(training_label.shape)\n",
    "        loss = criterion(predict_label, training_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #writer.add_scalar('Train Loss', loss.item(), epoch)\n",
    "        #writer.flush()\n",
    "        epoch_loss[epoch] += loss\n",
    "    epoch_loss[epoch] = epoch_loss[epoch] / i\n",
    "    schedular.step()\n",
    "    print(\"Average loss of epoch {} : Loss {:.8f}\".\n",
    "          format(epoch + 1, epoch_loss[epoch]))\n",
    "        \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\"Saving the Model\")\n",
    "        localtime = time.localtime(time.time())\n",
    "        torch.save(net.state_dict(), \n",
    "                  '/home/zhizuo/Landmark/models/FCN32s_{}_{}_{}_{}.pkl'.\n",
    "                  format(localtime[0], localtime[1], localtime[2], \n",
    "                         localtime[3], localtime[4]))\n",
    "        print(\"Successfully save the model of {}\".format(localtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, opt.max_epochs)\n",
    "epoch_loss = epoch_loss.data.numpy()\n",
    "plt.figure()\n",
    "l = plt.plot(x, epoch_loss, label = 'l2 loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN32s(\n",
       "  (pretrained_net): VGGNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (deconv1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FCN32s(pretrained_net = VGG, n_class = 1)\n",
    "net.load_state_dict(torch.load(\"/home/zhizuo/Landmark/models/FCN32s_2020_3_28_23.pkl\"))\n",
    "net.eval()\n",
    "net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# net = FCN32s(pretrained_net = VGG, n_class = 1)\n",
    "# net.load_state_dict(torch.load(\"/home/zhizuo/Landmark/models/FCN32s_2020_3_28_23.pkl\"))\n",
    "# net.eval()\n",
    "# net.cpu()\n",
    "\n",
    "idx = np.random.choice(range(int(numtotal*ratioTestTrain), numtotal))\n",
    "print(idx)\n",
    "testing_data = data[idx,[0,2,3]]\n",
    "testing_label = data[idx, 1]\n",
    "testing_data = torch.FloatTensor(testing_data)\n",
    "testing_data = testing_data.view(1,testing_data.shape[0], testing_data.shape[1], -1)\n",
    "\n",
    "predict_label = net(testing_data)\n",
    "predict_label = predict_label.view(predict_label.shape[2], -1)\n",
    "predict_label = predict_label.detach().numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, aspect = 'equal')\n",
    "for i in range(testing_label.shape[0]):\n",
    "    for j in range(testing_label.shape[1]):\n",
    "        if testing_data[0][0][i][j] == 0:\n",
    "            ax1.add_patch(patches.Rectangle((i/testing_label.shape[0], j/testing_label.shape[1]),\n",
    "                                           1/testing_label.shape[0], 1/testing_label.shape[1]))\n",
    "        if testing_label[i][j] == 1:\n",
    "            #print(\"fuck\")\n",
    "            #ax1.add_patch(patches.Rectangle((i/testing_label.shape[0], j/testing_label.shape[1]), \n",
    "                          #1/testing_label.shape[0], 1/testing_label.shape[1]))\n",
    "            plt.scatter(i/testing_label.shape[0], j/testing_label.shape[1], color = 'green', s=10)\n",
    "        if testing_data[0][1][i][j] == 1:\n",
    "            plt.scatter(i/testing_label.shape[0], j/testing_label.shape[1], color = 'red', s=10)\n",
    "        if testing_data[0][2][i][j] == 1:\n",
    "            plt.scatter(i/testing_label.shape[0], j/testing_label.shape[1], color = 'blue', s=10)\n",
    "        if predict_label[i][j] >= 0.1:\n",
    "            plt.scatter(i/testing_label.shape[0], j/testing_label.shape[1], color = 'orange', s=10)\n",
    "            \n",
    "\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of more complicated cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 160)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = FCN32s(pretrained_net = VGG, n_class = 1)\n",
    "net.load_state_dict(torch.load(\"/home/zhizuo/Landmark/models/FCN32s_2020_3_28_23.pkl\"))\n",
    "\n",
    "net.eval()\n",
    "net = net.cpu()\n",
    "test_x = np.array([1,1,1,0,1,1,1,0,1,1,\\\n",
    "                   1,0,1,0,1,1,1,0,1,1,\\\n",
    "                   1,1,1,1,1,0,1,1,1,1,\\\n",
    "                   1,1,1,0,1,1,1,0,1,1,\\\n",
    "                   1,1,1,0,1,1,1,0,1,1,\\\n",
    "                   0,1,0,0,0,1,0,0,0,0,\\\n",
    "                   1,1,1,0,1,1,1,0,1,1,\\\n",
    "                   1,1,1,1,1,0,1,1,1,0,\\\n",
    "                   1,0,1,0,1,1,1,0,1,1,\\\n",
    "                   1,1,1,0,1,1,1,0,1,1])\n",
    "test_x = test_x.reshape(10,10)\n",
    "init_x = np.zeros((10*opt.grid,10*opt.grid))\n",
    "init_x[10][10] = 1\n",
    "goal_x = np.zeros((10*opt.grid,10*opt.grid))\n",
    "goal_x[150][150] = 1\n",
    "test_x = test_x.repeat(opt.grid, axis = 0)\n",
    "test_x = test_x.repeat(opt.grid, axis = 1)\n",
    "\n",
    "test_input = []\n",
    "test_input.append(test_x)\n",
    "test_input.append(init_x)\n",
    "test_input.append(goal_x)\n",
    "test_input = np.array(test_input)\n",
    "test_input = test_input.reshape(1, 3, test_input.shape[2], -1)\n",
    "test_input = torch.FloatTensor(test_input)\n",
    "\n",
    "predict_label = net(test_input)\n",
    "predict_label = predict_label.view(predict_label.shape[2], -1)\n",
    "predict_label = predict_label.detach().numpy()\n",
    "print(predict_label.shape)\n",
    "\n",
    "\n",
    "dim = test_input.shape[2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, aspect = 'equal')\n",
    "for i in range(dim):\n",
    "    for j in range(dim):\n",
    "        #print(\"i, j = \", i, j)\n",
    "        if test_x[i][j] == 0:\n",
    "            ax1.add_patch(patches.Rectangle((i/dim, j/dim),\n",
    "                                           1/dim, 1/dim))\n",
    "        #if testing_label[i][j] == 1:\n",
    "            #print(\"fuck\")\n",
    "            #ax1.add_patch(patches.Rectangle((i/testing_label.shape[0], j/testing_label.shape[1]), \n",
    "                          #1/testing_label.shape[0], 1/testing_label.shape[1]))\n",
    "            #plt.scatter(i/testing_label.shape[0], j/dim, color = 'green', s=10)\n",
    "        if init_x[i][j] == 1:\n",
    "            plt.scatter(i/dim, j/dim, color = 'red', s=10)\n",
    "        if goal_x[i][j] == 1:\n",
    "            plt.scatter(i/dim, j/dim, color = 'blue', s=10)\n",
    "        if predict_label[i][j] >= 0.2:\n",
    "            plt.scatter(i/dim, j/dim, color = 'orange', s=10)\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
